{
  "hash": "410ac180fab7e081045c394abd4c5288",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Exploring Datasets\"\nsubtitle: \"\"\ndate: \"2024-05-14\"\nimage: ./002-Datasets.png\ncategories: [Intermediate, Data analysis, Python, Data Science, Machine Learning, Artificial Intelligence]\ndescription: \"The journey from raw data to a predictive model involves several crucial steps.\"\nauthor: \"Nicolas Tertusio\"\nexecute:\n  freeze: auto\nwebsite:\n  open-graph: true\n  title: \"Exploring Datasets\"\n  description: \"The journey from raw data to a predictive model involves several crucial steps.\"\n  image: ./002-Datasets.png\n  site-name: Nicolas Tertusio - Portfolio\nformat:\n    html:\n        toc: true\n        toc-depth: 3\n        toc-location: right\n        code-fold: false\n        code-summary: \"Code\"\n        code-copy: true\n        link-external-newwindow: true\n        other-links: \n            - text: Kaggle - AI & ML Community\n              href: https://www.kaggle.com/\n            - text: Credit Card Dataset\n              href: https://www.kaggle.com/datasets/tanayatipre/car-price-prediction-dataset\n        code-links:\n            - text: GitHub Link\n              href: https://github.com/Nicotertu/python-notebooks/blob/main/machine-learning/Exploring%20a%20dataset.ipynb\n              icon: file-code\n        pagetitle: Exploring Datasets\n---\n\n## Introduction\nIn the world of data science and machine learning, understanding the significance of not just diving headfirst into building models but rather meticulously exploring and understanding the datasets at hand is crucial.\n\nIn this article I embark on a journey through the pre-modeling phase, focusing on essential exploratory data analysis (EDA) techniques. I am going to be using a dataset from Kaggle, a website where one can find different datasets, people can upload their machine learning models, there's even competition and sometimes prizes for winners. The dataset is called \"Credit Car Prediction\", and it has multiple variables regarding the financial and social status of individuals. The goal of the dataset is to determine if an individual is or not eligible for a credit card.\n\nWe start off importing the libraries we are going to be using, and loading the dataset.\n\n::: {#9294e6d9 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MultipleLocator\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\n\ntrain_df = pd.read_csv(\"./002-train_data.csv\")\ntest_df = pd.read_csv(\"./002-test_data.csv\")\n```\n:::\n\n\n## Exploring the dataset\nLet's take a look at the dataset, see what types of features there are, what information they bring to the table, how many there are, how many entries and some of the statistics of each.\n\n::: {#fc80476b .cell execution_count=2}\n``` {.python .cell-code}\ntrain_df.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Gender</th>\n      <th>Has a car</th>\n      <th>Has a property</th>\n      <th>Children count</th>\n      <th>Income</th>\n      <th>Employment status</th>\n      <th>Education level</th>\n      <th>Marital status</th>\n      <th>Dwelling</th>\n      <th>Age</th>\n      <th>Employment length</th>\n      <th>Has a mobile phone</th>\n      <th>Has a work phone</th>\n      <th>Has a phone</th>\n      <th>Has an email</th>\n      <th>Job title</th>\n      <th>Family member count</th>\n      <th>Account age</th>\n      <th>Is high risk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5037048</td>\n      <td>M</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>135000.0</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>With parents</td>\n      <td>-16271</td>\n      <td>-3111</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Core staff</td>\n      <td>2.0</td>\n      <td>-17.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5044630</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>N</td>\n      <td>1</td>\n      <td>135000.0</td>\n      <td>Commercial associate</td>\n      <td>Higher education</td>\n      <td>Single / not married</td>\n      <td>House / apartment</td>\n      <td>-10130</td>\n      <td>-1651</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Accountants</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5079079</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>2</td>\n      <td>180000.0</td>\n      <td>Commercial associate</td>\n      <td>Secondary / secondary special</td>\n      <td>Married</td>\n      <td>House / apartment</td>\n      <td>-12821</td>\n      <td>-5657</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Laborers</td>\n      <td>4.0</td>\n      <td>-38.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5112872</td>\n      <td>F</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>360000.0</td>\n      <td>Commercial associate</td>\n      <td>Higher education</td>\n      <td>Single / not married</td>\n      <td>House / apartment</td>\n      <td>-20929</td>\n      <td>-2046</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Managers</td>\n      <td>1.0</td>\n      <td>-11.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5105858</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0</td>\n      <td>270000.0</td>\n      <td>Working</td>\n      <td>Secondary / secondary special</td>\n      <td>Separated</td>\n      <td>House / apartment</td>\n      <td>-16207</td>\n      <td>-515</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>-41.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<br />\n\n::: {#ff1a0a94 .cell execution_count=3}\n``` {.python .cell-code}\ntrain_df.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Children count</th>\n      <th>Income</th>\n      <th>Age</th>\n      <th>Employment length</th>\n      <th>Has a mobile phone</th>\n      <th>Has a work phone</th>\n      <th>Has a phone</th>\n      <th>Has an email</th>\n      <th>Family member count</th>\n      <th>Account age</th>\n      <th>Is high risk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.916500e+04</td>\n      <td>29165.000000</td>\n      <td>2.916500e+04</td>\n      <td>29165.000000</td>\n      <td>29165.000000</td>\n      <td>29165.0</td>\n      <td>29165.000000</td>\n      <td>29165.000000</td>\n      <td>29165.000000</td>\n      <td>29165.000000</td>\n      <td>29165.000000</td>\n      <td>29165.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.078232e+06</td>\n      <td>0.430790</td>\n      <td>1.868904e+05</td>\n      <td>-15979.477490</td>\n      <td>59257.761255</td>\n      <td>1.0</td>\n      <td>0.224310</td>\n      <td>0.294977</td>\n      <td>0.090279</td>\n      <td>2.197531</td>\n      <td>-26.137734</td>\n      <td>0.017110</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>4.182400e+04</td>\n      <td>0.741882</td>\n      <td>1.014096e+05</td>\n      <td>4202.997485</td>\n      <td>137655.883458</td>\n      <td>0.0</td>\n      <td>0.417134</td>\n      <td>0.456040</td>\n      <td>0.286587</td>\n      <td>0.912189</td>\n      <td>16.486702</td>\n      <td>0.129682</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>5.008804e+06</td>\n      <td>0.000000</td>\n      <td>2.700000e+04</td>\n      <td>-25152.000000</td>\n      <td>-15713.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>-60.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.042047e+06</td>\n      <td>0.000000</td>\n      <td>1.215000e+05</td>\n      <td>-19444.000000</td>\n      <td>-3153.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>-39.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.074666e+06</td>\n      <td>0.000000</td>\n      <td>1.575000e+05</td>\n      <td>-15565.000000</td>\n      <td>-1557.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>-24.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>5.114629e+06</td>\n      <td>1.000000</td>\n      <td>2.250000e+05</td>\n      <td>-12475.000000</td>\n      <td>-412.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>-12.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>5.150485e+06</td>\n      <td>19.000000</td>\n      <td>1.575000e+06</td>\n      <td>-7705.000000</td>\n      <td>365243.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>20.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<br />\n\n### Dropping features\nID can be quickly dropped, as it provides no information about the client. It is merely used by databases to assign a unique value to any given client.\n\n::: {#bf24b455 .cell execution_count=4}\n``` {.python .cell-code}\ntrain_df_step1 = train_df.drop(['ID'], axis=1)\n```\n:::\n\n\n<br />\nLooking at the mobile phones statistics:\n\n  - It has a mean of 1\n  - It has a standart deviation of 0\n  - It has a minimum of 1\n  - It has a maximum of 1\n  \nIt is simply a feature full of 1's, it provides no information. If we encountered in the validation set or real life a person with no mobile phone, the model can't possibly learn anything about them.\n\n::: {#8207b0ec .cell execution_count=5}\n``` {.python .cell-code}\ntrain_df_step2 = train_df_step1.drop(['Has a mobile phone'], axis=1)\n```\n:::\n\n\n<br />\n\n### Transforming categorical features\nFocusing on categorical features, there are three that we need to transform into discrete features. \n\n  - Gender we replace M to 0 and F to 1.\n  - Has a car we replace N to 0 and Y to 1.\n  - Has a property we replace N to 0 and Y to 1.\n\n::: {#d58f0556 .cell execution_count=6}\n``` {.python .cell-code}\n#replacing an entire column of strings for integers raises some warnings\n#with pd.option_context(\"future.no_silent_downcasting\", True): \ntrain_df_step2['Gender'] = train_df_step2['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)\ntrain_df_step2['Has a car'] = train_df_step2['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)\ntrain_df_step2['Has a property'] = train_df_step2['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)\n```\n:::\n\n\n<br />\n<div class=\"alert alert-warning\">\n<b>Warning:</b> Replacing an entire column of strings for integers raises some warnings. Consider using \"#with pd.option_context(\"future.no_silent_downcasting\", True):\" to remove the warning.\n</div>\n\nNext up are features that have more than two possible categories, like Employment status, Education level, Marital status, Dwelling and Job title. \n\nPlotting them is a very helpful way to visualize how many categories there are and the population of each. See @fig-uniques1, @fig-uniques2, @fig-uniques3, @fig-uniques4, @fig-uniques5.\n\n::: {#163c1057 .cell execution_count=7}\n``` {.python .cell-code}\nfeatures = ['Employment status', 'Education level', 'Marital status', 'Dwelling', 'Job title']\n\nfor feature in features:\n    plt.figure(figsize=(7, 4))\n    sns.countplot(data=train_df_step2, x=feature)\n    plt.title(f'Count of Unique Values in {feature}')\n    plt.xlabel('')\n    plt.xticks(rotation=90)\n    plt.ylabel('Counts')\n    \nplt.show()\n```\n:::\n\n\n::: {#cell-fig-uniques1 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![Unique values for Employment status](002-Datasets_files/figure-html/fig-uniques1-output-1.png){#fig-uniques1 width=610 height=491 fig-align='center'}\n:::\n:::\n\n\n::: {#cell-fig-uniques2 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![Unique values for Education level](002-Datasets_files/figure-html/fig-uniques2-output-1.png){#fig-uniques2 width=610 height=547 fig-align='center'}\n:::\n:::\n\n\n::: {#cell-fig-uniques3 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![Unique values for Marital status](002-Datasets_files/figure-html/fig-uniques3-output-1.png){#fig-uniques3 width=610 height=476 fig-align='center'}\n:::\n:::\n\n\n::: {#cell-fig-uniques4 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![Unique values for Dwelling](002-Datasets_files/figure-html/fig-uniques4-output-1.png){#fig-uniques4 width=610 height=482 fig-align='center'}\n:::\n:::\n\n\n::: {#cell-fig-uniques5 .cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![Unique values for Job title](002-Datasets_files/figure-html/fig-uniques5-output-1.png){#fig-uniques5 width=601 height=484 fig-align='center'}\n:::\n:::\n\n\n<br />\nJob title has NaN values though, are they all students and pensioners that don't work anymore? Or are there no job title for some people's jobs? Let's combine Employment status and Job title, so we can see what the employment status is of those whose job title is missing, see @fig-job-titles.\n\n::: {#cell-fig-job-titles .cell execution_count=13}\n``` {.python .cell-code}\nnan_job_title_df = train_df[train_df['Job title'].isna()]\nemployment_status_counts = nan_job_title_df['Employment status'].value_counts()\n\nplt.figure(figsize=(8, 6))\nplt.pie(employment_status_counts, labels=employment_status_counts.index, autopct='%1.1f%%', startangle=45)\nplt.title('Employment Status for NaN Job Titles')\nplt.axis('equal')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Employment Status for NaN Job Titles](002-Datasets_files/figure-html/fig-job-titles-output-1.png){#fig-job-titles width=685 height=483 fig-align='center'}\n:::\n:::\n\n\n<br />\nIt's all around the place. Gathering the information we have so far:\n\n  - Job title has 9027 NaN values (roughly 30%)\n  - Missing job titles seem to be all over the place compared their employment status, MCAR (Missing Completely at Random)\n  - Imputing would be complicated (but not impossible)\n  - Encoding this feature would increase the dimensionality significantly\n\nBased on that, I will remove the feature altogether, and apply OHE (One Hot Encoding).\n\n::: {#2c20d212 .cell execution_count=14}\n``` {.python .cell-code}\ntrain_df_step3 = train_df_step2.drop(['Job title'], axis=1)\n\ntrain_df_step4 = pd.get_dummies(train_df_step3, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)\n```\n:::\n\n\n<br />\n\n### Correlation matrix\nFinally, let's take a look at the correlation matrix.\n\n::: {#cell-fig-correlation-matrix .cell execution_count=15}\n``` {.python .cell-code}\ncorrelation_matrix = train_df_step4.corr()\n\nplt.figure(figsize=(25,25))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation matrix')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Correlation matrix](002-Datasets_files/figure-html/fig-correlation-matrix-output-1.png){#fig-correlation-matrix width=2064 height=2208 fig-align='center'}\n:::\n:::\n\n\n<br />\nThere are two things that might be interesting to explore further: \n  - Correlation between 'Family member count' and 'Children count'.\n  - Correlation between 'Employment status_Pensioner' and 'Employment length'.\n\n'Family member count' and 'Children count' are obviously correlated, but it is not a correlation of 1. That means that sometimes, more family members taken into account other than children, or even sometimes children are not taken into account for family, therefore we can't really remove this feature. It is adding some information after all, see @fig-family-children-count.\n\n::: {#cell-fig-family-children-count .cell execution_count=16}\n``` {.python .cell-code}\nfamily_member_count = train_df_step4['Family member count']\nchildren_count = train_df_step4['Children count']\n\nplt.figure(figsize=(8, 6))\nplt.scatter(family_member_count, children_count, alpha=0.5)\nplt.title('Scatter Plot of Family Member Count vs Children Count')\nplt.xlabel('Family Member Count')\nplt.ylabel('Children Count')\nplt.grid(True)\n\nplt.gca().xaxis.set_major_locator(MultipleLocator(1))\nplt.gca().yaxis.set_major_locator(MultipleLocator(1))\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Correlation matrix](002-Datasets_files/figure-html/fig-family-children-count-output-1.png){#fig-family-children-count width=659 height=523 fig-align='center'}\n:::\n:::\n\n\n<br />\nWe can see that most of the time, children make up for most of the family members, but sometimes (especifically for 1 family member), there are a few cases with more children than family members.\n\nMoving to the next point to explore, let's take a look at those with an employment status of pensioner and their respective 'Employment length'.\n\n::: {#70d6f486 .cell execution_count=17}\n``` {.python .cell-code}\nemployment_status_pensioner = train_df_step4[(train_df_step4['Employment status_Pensioner'] == 1)]\nemployment_status_pensioner['Employment length'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\nEmployment length\n 365243    4908\n-678          1\n-443          1\n-586          1\n-672          1\n-673          1\n-3680         1\n-2745         1\n-2208         1\n-620          1\n-1325         1\n-1644         1\n-2269         1\nName: count, dtype: int64\n```\n:::\n:::\n\n\n<br />\nOut of 4920 pensioners, only 12 have an 'Employment length' different than the value '365243'. That is a 0.24% over the pensioners and 0.04% over the dataset. It is most likely safe to remove one of these two features, as they are explaining the same thing. But which one to remove matters: 'Employment status_Pensioner' has the same information as 'Employment length', but not the other way around. 'Employment length' has information about those that are not Pensioners.\n\nThat means we can only remove the Employment status_Pensioner feature.\n\n::: {#0c4db214 .cell execution_count=18}\n``` {.python .cell-code}\ntrain_df_step5 = train_df_step4.drop(['Employment status_Pensioner'], axis=1)\n```\n:::\n\n\n<br />\n\n### Scaling features\nAt last, before training a machine learning model, we should scale features, given that some of them like 'Income' and 'Employment length' are not proportional to the others.\n\n::: {#b586bca8 .cell execution_count=19}\n``` {.python .cell-code}\nscaler = MinMaxScaler()\ntrain_df_step6 = scaler.fit_transform(train_df_step5)\ntrain_df_step6 = pd.DataFrame(train_df_step6, columns=train_df_step5.columns)\n```\n:::\n\n\n<br />\n\n## Training a model\nTo train a model we first split the data in input (x_data) and output (y_data). Then we apply the following: \n  - Split dataset between train and test sets.\n  - Apply cross validation\n  - Search for best hyperparameters\n  - Make predictions\n  - Calculate accuracy\n\nTo visually see the results, we can use a confusion matrix.\n\n::: {#fig-confusion-matrix-1 .cell execution_count=20}\n``` {.python .cell-code}\ny_data = train_df_step6['Is high risk']\nx_data = train_df_step6.drop(columns=['Is high risk'])\n\ndef train_model(x, y):\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    rf_classifier = RandomForestClassifier(random_state=42)\n    param_grid = {\n        'n_estimators': [10, 100],\n        'max_depth': [None, 5, 10],\n        'min_samples_split': [2, 4],\n        'min_samples_leaf': [1, 3]\n    }\n\n    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n    grid_search.fit(X_train, y_train)\n\n    best_params = grid_search.best_params_\n    print(\"Best hyperparameters:\", best_params)\n    best_model = grid_search.best_estimator_\n\n    y_pred = best_model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy on testing set:\", accuracy)\n    \n    y_pred = best_model.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.show()\n    \n    return best_model\n    \nbest_model = train_model(x_data, y_data)\n```\n:::\n\n\n<br />\n98.52% accuracy is great! That meanst almost every observation we made about the dataset were appropiate and helped us build an accurate model... or not.\n\nSome machine learning models, especially those that are tree based, tend to fall into a trap of unbalanced classes. And in this dataset, there's a huge unbalance in our output 'Is high risk'.\n\n::: {#f830daa4 .cell execution_count=21}\n``` {.python .cell-code}\ntrain_df_step6['Is high risk'].value_counts()\n```\n:::\n\n\n<br />\nOnly 1.74% are high risk. What does that mean? Well... instead of analyzing the dataset, evaluating models, hyperparameters, cross validation and whatnot, it would be more effective to say \"No client is high risk\". That would give us an accuracy of 98.26%. Looking at the confusion matrix, that is exactly what the model learned and did.\n<br />\n\n### Balancing classes\nOne way to fix this is to balance the classes, either by undersampling or oversampling. Let's try and use oversampling, as we don't really want to loose 28000 entries of valuable information.\n\n::: {#3b9df208 .cell execution_count=22}\n``` {.python .cell-code}\nsmote = SMOTE()\n\nx_smote, y_smote = smote.fit_resample(x_data, y_data)\n\nprint('Original dataset shape')\nprint(y_data.value_counts())\nprint('\\nResample dataset shape')\nprint(y_smote.value_counts())\n```\n:::\n\n\n<br />\nNow classes are balanced, let's train a new model... a balanced one.\n\n::: {#87b48381 .cell execution_count=23}\n``` {.python .cell-code}\nbest_model_balanced = train_model(x_smote, y_smote)\n```\n:::\n\n\n<br />\n\n### Comparing balanced and unbalanced models\nAnd now, after balancing our classes, we get an even better accuracy, 99.03%! But it wouldn't be fair to compare when we artificially created almost half the data. Let's compare both models, the balanced and unbalanced models, with the entire dataset as input.\n\n::: {#fdc0623e .cell execution_count=24}\n``` {.python .cell-code}\ny_pred_balanced = best_model_balanced.predict(x_data)\nbalanced_accuracy = accuracy_score(y_data, y_pred_balanced)\n\ncm = confusion_matrix(y_data, y_pred_balanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\n```\n:::\n\n\n<br />\n\n::: {#ab8008f9 .cell execution_count=25}\n``` {.python .cell-code}\ny_pred_unbalanced = best_model.predict(x_data)\nunbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)\n\ncm = confusion_matrix(y_data, y_pred_unbalanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\n```\n:::\n\n\n<br />\nResults for the balanced model:\n  - It predicted that 436 cases were high risk, 387 of those were rightfully high risk (88.76% accuracy).\n  - It predicted that 28729 cases were not high risk, 28617 of those were rightfully not high risk (99.61% accuracy).\n  - Overall accuracy of 99.45%.\n\nResults for the unbalanced mode: \n  - It predicted that 3 cases were high risk, 3 of those were rightfully high risk (100% accuracy).\n  - It predicted that 29162 cases were not high risk, 28666 of those were rightfully not high risk (98.30% accuracy).\n  - Overall accuracy of 98.30%.\n\nOnly when measured by their overall accuracy does the balanced model truly shine. But only a glimpse into the confusion matrix will clearly shows that the balanced model actually learned from the high risk entries, whereas the unbalanced one barely did.\n\n### Comparing with unseen data\nIn all fairness though, we're measuring them by almost the same data we trained both models on. Let's throw at them data it they have never seen.\n\n::: {#d49c66fc .cell execution_count=26}\n``` {.python .cell-code}\ntest_df = pd.read_csv(\"./test_data.csv\")\ntest_df = pd.get_dummies(test_df, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)\ntest_df = test_df.drop(['ID','Has a mobile phone','Job title','Employment status_Pensioner'], axis=1)\nwith pd.option_context(\"future.no_silent_downcasting\", True):\n    test_df['Gender'] = test_df['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)\n    test_df['Has a property'] = test_df['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)\n    test_df['Has a car'] = test_df['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)\nscaler = MinMaxScaler()\ntest_df_columns = test_df.columns\ntest_df = scaler.fit_transform(test_df)\ntest_df = pd.DataFrame(test_df, columns = test_df_columns)\n```\n:::\n\n\n<br />\n\n::: {#5f203ab7 .cell execution_count=27}\n``` {.python .cell-code}\ny_data = test_df['Is high risk']\nx_data = test_df.drop(columns=['Is high risk'])\n\ny_pred_unbalanced = best_model.predict(x_data)\nunbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)\n\ny_pred_balanced = best_model_balanced.predict(x_data)\nbalanced_accuracy = accuracy_score(y_data, y_pred_balanced)\n\nprint(f\"Accuracy on dataset with balanced training: {balanced_accuracy*100:.2f}%\")\nprint(f\"Accuracy on dataset set with unbalanced training: {unbalanced_accuracy*100:.2f}%\")\n```\n:::\n\n\n<br />\nOverall accuracy of the balanced model dropped to 90.65% and the unbalanced model stayed high at 98.40%, but is it truly accurate?\n\n::: {#a9f67699 .cell execution_count=28}\n``` {.python .cell-code}\ncm = confusion_matrix(y_data, y_pred_unbalanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Unbalanced Confusion Matrix')\nplt.show()\n\ncm = confusion_matrix(y_data, y_pred_balanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Balanced Confusion Matrix')\nplt.show()\n```\n:::\n\n\n<br />\nIt isn't. The unbalanced model did not predict any high risk, whereas the balanced model predicted 635 cases. It did so correctly at 5.51% accuracy which is not that good, but that is a story for a different day.\n\n",
    "supporting": [
      "002-Datasets_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}
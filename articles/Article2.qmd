---
title: "Exploring Datasets"
subtitle: ""
date: "2024-05-14"
image: ./Article2.png
categories: [Intermediate, Data analysis, Python, Data Science, Machine Learning, Artificial Intelligence]
description: "The journey from raw data to a predictive model involves several crucial steps."
author: "Nicolas Tertusio"
website:
  open-graph: true
  title: "Learning how to use Markdown in Jupyter Notebooks"
  description: "The journey from raw data to a predictive model involves several crucial steps."
  image: ./Datasets.png
  site-name: Nicolas Tertusio - Portfolio
format:
    html:
        toc: true
        toc-depth: 3
        toc-location: left
        code-fold: true
        code-summary: "See code"
        code-copy: true
        link-external-newwindow: true
        other-links: 
            - text: Kaggle - AI & ML Community
              href: https://www.kaggle.com/
            - text: Credit Card Dataset
              href: https://www.kaggle.com/datasets/tanayatipre/car-price-prediction-dataset
        code-links:
            - text: GitHub Link
              href: https://github.com/Nicotertu/python-notebooks/blob/main/machine-learning/Exploring%20a%20dataset.ipynb
              icon: file-code
        pagetitle: Exploring Datasets
---

In the world of data science and machine learning, understanding the significance of not just diving headfirst into building models but rather meticulously exploring and understanding the datasets at hand is crucial.

In this article I embark on a journey through the pre-modeling phase, focusing on essential exploratory data analysis (EDA) techniques. I am going to be using a dataset from Kaggle, a website where one can find different datasets, people can upload their machine learning models, there's even competition and sometimes prizes for winners. The dataset is called "Credit Car Prediction", and it has multiple variables regarding the financial and social status of individuals. The goal of the dataset is to determine if an individual is or not eligible for a credit card.

We start off importing the libraries we are going to be using, and loading the dataset.

``` {python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import MultipleLocator
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE

train_df = pd.read_csv("./train_data.csv")
test_df = pd.read_csv("./test_data.csv")
```

Let's take a look at the dataset, see what types of features there are, what information they bring to the table, how many there are, how many entries and some of the statistics of each.

```{python}
train_df.head(5)
```

```{python}
train_df.describe()
```

ID can be quickly eliminated, as it provides no information about the client. It is merely used by databases to assign a unique value to any given client.

```{python}
train_df_step1 = train_df.drop(['ID'], axis=1)
```

Looking at mobile phones, it has a mean of 1, standart deviation of 0, minimum of 1 and maximum of 1. It is simply a feature full of 1's, it provides no information. If we encountered in the validation set or real life a person with no mobile phone, the model can't possibly learn anything about them.

```{python}
train_df_step2 = train_df_step1.drop(['Has a mobile phone'], axis=1)
```

Focusing on categorical features, there are three that we need to transform into discrete features. 
  - Gender we replace M to 0 and F to 1.
  - Has a car we replace N to 0 and Y to 1.
  - Has a property we replace N to 0 and Y to 1.

```{python}
#replacing an entire column of strings for integers raises some warnings
with pd.option_context("future.no_silent_downcasting", True): 
    train_df_step2['Gender'] = train_df_step2['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)
    train_df_step2['Has a car'] = train_df_step2['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)
    train_df_step2['Has a property'] = train_df_step2['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)
```

Next up are features that have more than two possible categories, like Employment status, Education level, Marital status, Dwelling and Job title. 

Plotting them is a very helpful way to visualize how many categories there are and the population of each.

```{python}
features = ['Employment status', 'Education level', 'Marital status', 'Dwelling', 'Job title']

for feature in features:
    plt.figure(figsize=(10, 6))
    sns.countplot(data=train_df_step2, x=feature)
    plt.title(f'Count of Unique Values in {feature}')
    plt.xlabel('')
    plt.xticks(rotation=90)
    plt.ylabel('Counts')
    
plt.show()
```

Job title has NaN values though, are they all students and pensioners that don't work anymore? Or are there no job title for some people's jobs? Let's combine Employment status and Job title, so we can see what the employment status is of those whose job title is missing.

```{python}
nan_job_title_df = train_df[train_df['Job title'].isna()]
employment_status_counts = nan_job_title_df['Employment status'].value_counts()

plt.figure(figsize=(8, 6))
plt.pie(employment_status_counts, labels=employment_status_counts.index, autopct='%1.1f%%', startangle=45)
plt.title('Employment Status for NaN Job Titles')
plt.axis('equal')
plt.show()
```

It's all around the place. Gathering the information we have so far:
  - Job title has 9027 NaN values (roughly 30%)
  - Missing job titles seem to be all over the place compared their employment status, MCAR (Missing Completely at Random)
  - Imputing would be complicated (but not impossible)
  - Encoding this feature would increase the dimensionality significantly

Based on that, I will remove the feature altogether.

```{python}
train_df_step3 = train_df_step2.drop(['Job title'], axis=1)

train_df_step4 = pd.get_dummies(train_df_step3, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)
```

```{python}
correlation_matrix = train_df_step4.corr()

plt.figure(figsize=(25,25))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation matrix')
plt.show()
```

```{python}
family_member_count = train_df_step4['Family member count']
children_count = train_df_step4['Children count']

plt.figure(figsize=(8, 6))
plt.scatter(family_member_count, children_count, alpha=0.5)
plt.title('Scatter Plot of Family Member Count vs Children Count')
plt.xlabel('Family Member Count')
plt.ylabel('Children Count')
plt.grid(True)

plt.gca().xaxis.set_major_locator(MultipleLocator(1))
plt.gca().yaxis.set_major_locator(MultipleLocator(1))

plt.show()
```

```{python}
employment_status_pensioner = train_df_step4[(train_df_step4['Employment status_Pensioner'] == 1)]
employment_status_pensioner['Employment length'].value_counts()
```

```{python}
train_df_step5 = train_df_step4.drop(['Employment status_Pensioner'], axis=1)
```

```{python}
scaler = MinMaxScaler()
train_df_step6 = scaler.fit_transform(train_df_step5)
train_df_step6 = pd.DataFrame(train_df_step6, columns=train_df_step5.columns)
```

```{python}
y_data = train_df_step6['Is high risk']
x_data = train_df_step6.drop(columns=['Is high risk'])

def train_model(x, y):
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    rf_classifier = RandomForestClassifier(random_state=42)
    param_grid = {
        'n_estimators': [10, 100],
        'max_depth': [None, 5, 10],
        'min_samples_split': [2, 4],
        'min_samples_leaf': [1, 3]
    }

    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)

    best_params = grid_search.best_params_
    print("Best hyperparameters:", best_params)
    best_model = grid_search.best_estimator_

    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy on testing set:", accuracy)
    
    y_pred = best_model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred)

    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()
    
    return best_model
    
best_model = train_model(x_data, y_data)
```

```{python}
train_df_step6['Is high risk'].value_counts()
```

```{python}
smote = SMOTE()

x_smote, y_smote = smote.fit_resample(x_data, y_data)

print('Original dataset shape')
print(y_data.value_counts())
print('\nResample dataset shape')
print(y_smote.value_counts())
```

```{python}
best_model_balanced = train_model(x_smote, y_smote)
```

```{python}
y_pred_balanced = best_model_balanced.predict(x_data)
balanced_accuracy = accuracy_score(y_data, y_pred_balanced)

cm = confusion_matrix(y_data, y_pred_balanced)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()
```

```{python}
y_pred_unbalanced = best_model.predict(x_data)
unbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)

cm = confusion_matrix(y_data, y_pred_unbalanced)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()
```

```{python}
test_df = pd.read_csv("./test_data.csv")
test_df = pd.get_dummies(test_df, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)
test_df = test_df.drop(['ID','Has a mobile phone','Job title','Employment status_Pensioner'], axis=1)
with pd.option_context("future.no_silent_downcasting", True):
    test_df['Gender'] = test_df['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)
    test_df['Has a property'] = test_df['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)
    test_df['Has a car'] = test_df['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)
scaler = MinMaxScaler()
test_df_columns = test_df.columns
test_df = scaler.fit_transform(test_df)
test_df = pd.DataFrame(test_df, columns = test_df_columns)
```

```{python}
y_data = test_df['Is high risk']
x_data = test_df.drop(columns=['Is high risk'])

y_pred_unbalanced = best_model.predict(x_data)
unbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)

y_pred_balanced = best_model_balanced.predict(x_data)
balanced_accuracy = accuracy_score(y_data, y_pred_balanced)

print(f"Accuracy on dataset with balanced training: {balanced_accuracy:.4f}%")
print(f"Accuracy on dataset set with unbalanced training: {unbalanced_accuracy:.4f}%")
```
---
title: "Exploring Datasets"
subtitle: ""
date: "2024-05-14"
image: ./Article2.png
categories: [Intermediate, Data analysis, Python, Data Science, Machine Learning, Artificial Intelligence]
description: "The journey from raw data to a predictive model involves several crucial steps."
author: "Nicolas Tertusio"
website:
  open-graph: true
  title: "Learning how to use Markdown in Jupyter Notebooks"
  description: "The journey from raw data to a predictive model involves several crucial steps."
  image: ./Datasets.png
  site-name: Nicolas Tertusio - Portfolio
format:
    html:
        toc: true
        toc-depth: 3
        toc-location: left
        code-fold: true
        code-summary: "See code"
        code-copy: true
        link-external-newwindow: true
        other-links: 
            - text: Kaggle - AI & ML Community
              href: https://www.kaggle.com/
            - text: Credit Card Dataset
              href: https://www.kaggle.com/datasets/tanayatipre/car-price-prediction-dataset
        code-links:
            - text: GitHub Link
              href: https://github.com/Nicotertu/python-notebooks/blob/main/machine-learning/Exploring%20a%20dataset.ipynb
              icon: file-code
        pagetitle: Exploring Datasets
---

In the world of data science and machine learning, understanding the significance of not just diving headfirst into building models but rather meticulously exploring and understanding the datasets at hand is crucial.

In this article I embark on a journey through the pre-modeling phase, focusing on essential exploratory data analysis (EDA) techniques. I am going to be using a dataset from Kaggle, a website where one can find different datasets, people can upload their machine learning models, there's even competition and sometimes prizes for winners. The dataset is called "Credit Car Prediction", and it has multiple variables regarding the financial and social status of individuals. The goal of the dataset is to determine if an individual is or not eligible for a credit card.

We start off importing the libraries we are going to be using, and loading the dataset.

``` {python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import MultipleLocator
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE

train_df = pd.read_csv("./train_data.csv")
test_df = pd.read_csv("./test_data.csv")
```

Let's take a look at the dataset, see what types of variables there are, what information they bring to the table, how many there are, and how many entries.

```{python}
train_df.head(5)
```



```{python}
def corr_matrix(df):
    correlation_matrix = df.corr()

    plt.figure(figsize=(25,25))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
    plt.title('Correlation matrix')
    plt.show()
```



```{python}
train_df_step1 = train_df.drop(['ID'], axis=1)
```

```{python}
train_df_step2 = train_df_step1.drop(['Has a mobile phone'], axis=1)
```

```{python}
#replacing an entire column of strings for integers raises some warnings
with pd.option_context("future.no_silent_downcasting", True): 
    train_df_step2['Gender'] = train_df_step2['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)
    train_df_step2['Has a car'] = train_df_step2['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)
    train_df_step2['Has a property'] = train_df_step2['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)
```

```{python}
print(f"Employment status: {train_df['Employment status'].unique()}\n")
print(f"Education level: {train_df['Education level'].unique()}\n")
print(f"Marital status: {train_df['Marital status'].unique()}\n")
print(f"Dwelling: {train_df['Dwelling'].unique()}\n")
print(f"Job title: {train_df['Job title'].unique()}")
```

```{python}
nan_job_df = train_df[train_df['Job title'].isna()]
print(f"Employment status: {nan_job_df['Employment status'].unique()}\n")
```

```{python}
train_df_step3 = train_df_step2.drop(['Job title'], axis=1)

train_df_step4 = pd.get_dummies(train_df_step3, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)
```

```{python}
corr_matrix(train_df_step4)
```

```{python}
has_a_car = train_df_step4['Has a car']
has_a_property = train_df_step4['Has a property']

plt.figure(figsize=(8, 6))
plt.scatter(has_a_property, has_a_car)
plt.title('Scatter Plot of Has a car vs Has a property')
plt.xlabel('Has a property')
plt.ylabel('Has a car')
plt.grid(True)
plt.show()
```

```{python}
train_df_step5 = train_df_step4.drop(['Has a car'], axis=1)
```

```{python}
family_member_count = train_df_step5['Family member count']
children_count = train_df_step5['Children count']

plt.figure(figsize=(8, 6))
plt.scatter(family_member_count, children_count, alpha=0.5)
plt.title('Scatter Plot of Family Member Count vs Children Count')
plt.xlabel('Family Member Count')
plt.ylabel('Children Count')
plt.grid(True)

plt.gca().xaxis.set_major_locator(MultipleLocator(1))
plt.gca().yaxis.set_major_locator(MultipleLocator(1))

plt.show()
```

```{python}
employment_status_pensioner = train_df_step5[(train_df_step5['Employment status_Pensioner'] == 1)]
employment_status_pensioner['Employment length'].value_counts()
```

```{python}
train_df_step6 = train_df_step5.drop(['Employment status_Pensioner'], axis=1)
```

```{python}
scaler = MinMaxScaler()
train_df_step7 = scaler.fit_transform(train_df_step6)
train_df_step7 = pd.DataFrame(train_df_step7, columns=train_df_step6.columns)
```

```{python}
y_data = train_df_step7['Is high risk']
x_data = train_df_step7.drop(columns=['Is high risk'])

def train_model(x, y):
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    rf_classifier = RandomForestClassifier()
    param_grid = {
        'n_estimators': [10, 50, 100],
        'max_depth': [None, 5, 10],
        'min_samples_split': [2, 4],
        'min_samples_leaf': [1, 3]
    }

    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)

    best_params = grid_search.best_params_
    print("Best hyperparameters ():", best_params)
    best_model = grid_search.best_estimator_

    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy on testing set:", accuracy)
    
    # Get predictions on the test set
    y_pred = best_model.predict(X_test)

    # Generate confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    # Plot confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()
    
    return best_model
    
best_model = train_model(x_data, y_data)
```

```{python}
train_df_step7['Is high risk'].value_counts()
```

```{python}
smote = SMOTE()

x_smote, y_smote = smote.fit_resample(x_data, y_data)

print('Original dataset shape')
print(y_data.value_counts())
print('\nResample dataset shape')
print(y_smote.value_counts())
```

```{python}
best_model_balanced = train_model(x_smote, y_smote)
```

```{python}
y_pred_balanced = best_model_balanced.predict(x_data)
balanced_accuracy = accuracy_score(y_data, y_pred_balanced)

# Generate confusion matrix
cm = confusion_matrix(y_data, y_pred_balanced)

# Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()
```

```{python}
y_pred_unbalanced = best_model.predict(x_data)
unbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)

# Generate confusion matrix
cm = confusion_matrix(y_data, y_pred_unbalanced)

# Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()
```

```{python}
print(f"Accuracy on testing set with balanced training: {balanced_accuracy:.4f}%")
print(f"Accuracy on testing set with unbalanced training: {unbalanced_accuracy:.4f}%")
```

```{python}
```
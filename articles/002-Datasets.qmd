---
title: "Exploring Datasets"
subtitle: ""
date: "2024-05-14"
image: ./002-Datasets.png
categories: [Intermediate, Data analysis, Python, Data Science, Machine Learning, Artificial Intelligence]
description: "The journey from raw data to a predictive model involves several crucial steps."
author: "Nicolas Tertusio"
execute:
  freeze: auto
website:
  open-graph: true
  title: "Exploring Datasets"
  description: "The journey from raw data to a predictive model involves several crucial steps."
  image: ./002-Datasets.png
  site-name: Nicolas Tertusio - Portfolio
format:
    html:
        toc: true
        toc-depth: 3
        toc-location: right
        code-fold: false
        code-summary: "Code"
        code-copy: true
        link-external-newwindow: true
        other-links: 
            - text: Kaggle - AI & ML Community
              href: https://www.kaggle.com/
            - text: Credit Card Dataset
              href: https://www.kaggle.com/datasets/tanayatipre/car-price-prediction-dataset
        code-links:
            - text: GitHub Link
              href: https://github.com/Nicotertu/python-notebooks/blob/main/machine-learning/Exploring%20a%20dataset.ipynb
              icon: file-code
        pagetitle: Exploring Datasets
---

## Introduction
In the world of data science and machine learning, understanding the significance of not just diving headfirst into building models but rather meticulously exploring and understanding the datasets at hand is crucial.

In this article I embark on a journey through the pre-modeling phase, focusing on essential exploratory data analysis (EDA) techniques. I am going to be using a dataset from Kaggle, a website where one can find different datasets, people can upload their machine learning models, there's even competition and sometimes prizes for winners. The dataset is called "Credit Car Prediction", and it has multiple variables regarding the financial and social status of individuals. The goal of the dataset is to determine if an individual is or not eligible for a credit card.

We start off importing the libraries we are going to be using, and loading the dataset.

``` {python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import MultipleLocator
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE

train_df = pd.read_csv("./002-train_data.csv")
test_df = pd.read_csv("./002-test_data.csv")
```
## Exploring the dataset
Let's take a look at the dataset, see what types of features there are, what information they bring to the table, how many there are, how many entries and some of the statistics of each.

```{python}
train_df.head(5)
```
<br />

```{python}
train_df.describe()
```
<br />

### Dropping features
ID can be quickly dropped, as it provides no information about the client. It is merely used by databases to assign a unique value to any given client.

```{python}
train_df_step1 = train_df.drop(['ID'], axis=1)
```
<br />
Looking at the mobile phones statistics:

  - It has a mean of 1
  - It has a standart deviation of 0
  - It has a minimum of 1
  - It has a maximum of 1
  
It is simply a feature full of 1's, it provides no information. If we encountered in the validation set or real life a person with no mobile phone, the model can't possibly learn anything about them.

```{python}
train_df_step2 = train_df_step1.drop(['Has a mobile phone'], axis=1)
```
<br />

### Transforming categorical features
Focusing on categorical features, there are three that we need to transform into discrete features. 

  - Gender we replace M to 0 and F to 1.
  - Has a car we replace N to 0 and Y to 1.
  - Has a property we replace N to 0 and Y to 1.

```{python}
#replacing an entire column of strings for integers raises some warnings
#with pd.option_context("future.no_silent_downcasting", True): 
train_df_step2['Gender'] = train_df_step2['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)
train_df_step2['Has a car'] = train_df_step2['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)
train_df_step2['Has a property'] = train_df_step2['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)
```
<br />
<div class="alert alert-warning">
<b>Warning:</b> Replacing an entire column of strings for integers raises some warnings. Consider using "#with pd.option_context("future.no_silent_downcasting", True):" to remove the warning.
</div>

Next up are features that have more than two possible categories, like Employment status, Education level, Marital status, Dwelling and Job title. 

Plotting them is a very helpful way to visualize how many categories there are and the population of each. See @fig-uniques1, @fig-uniques2, @fig-uniques3, @fig-uniques4, @fig-uniques5.
```{python}
#| eval: false
features = ['Employment status', 'Education level', 'Marital status', 'Dwelling', 'Job title']

for feature in features:
    plt.figure(figsize=(7, 4))
    sns.countplot(data=train_df_step2, x=feature)
    plt.title(f'Count of Unique Values in {feature}')
    plt.xlabel('')
    plt.xticks(rotation=90)
    plt.ylabel('Counts')
    
plt.show()
```

```{python}
#| echo: false
#| label: fig-uniques1
#| fig-cap: Unique values for Employment status
#| fig-align: center
plt.figure(figsize=(7, 4))
sns.countplot(data=train_df_step2, x='Employment status')
plt.title(f'Count of Unique Values in Employment status')
plt.xlabel('')
plt.xticks(rotation=90)
plt.ylabel('Counts')
plt.show()
```

```{python}
#| echo: false
#| label: fig-uniques2
#| fig-cap: Unique values for Education level
#| fig-align: center
plt.figure(figsize=(7, 4))
sns.countplot(data=train_df_step2, x='Education level')
plt.title(f'Count of Unique Values in Education level')
plt.xlabel('')
plt.xticks(rotation=90)
plt.ylabel('Counts')
plt.show()
```

```{python}
#| echo: false
#| label: fig-uniques3
#| fig-cap: Unique values for Marital status
#| fig-align: center
plt.figure(figsize=(7, 4))
sns.countplot(data=train_df_step2, x='Marital status')
plt.title(f'Count of Unique Values in Marital status')
plt.xlabel('')
plt.xticks(rotation=90)
plt.ylabel('Counts')
plt.show()
```

```{python}
#| echo: false
#| label: fig-uniques4
#| fig-cap: Unique values for Dwelling
#| fig-align: center
plt.figure(figsize=(7, 4))
sns.countplot(data=train_df_step2, x='Dwelling')
plt.title(f'Count of Unique Values in Dwelling')
plt.xlabel('')
plt.xticks(rotation=90)
plt.ylabel('Counts')
plt.show()
```

```{python}
#| echo: false
#| label: fig-uniques5
#| fig-cap: Unique values for Job title
#| fig-align: center
plt.figure(figsize=(7, 4))
sns.countplot(data=train_df_step2, x='Job title')
plt.title(f'Count of Unique Values in Job title')
plt.xlabel('')
plt.xticks(rotation=90)
plt.ylabel('Counts')
plt.show()
```
<br />
Job title has NaN values though, are they all students and pensioners that don't work anymore? Or are there no job title for some people's jobs? Let's combine Employment status and Job title, so we can see what the employment status is of those whose job title is missing, see @fig-job-titles.

```{python}
#| label: fig-job-titles
#| fig-cap: Employment Status for NaN Job Titles
#| fig-align: center
nan_job_title_df = train_df[train_df['Job title'].isna()]
employment_status_counts = nan_job_title_df['Employment status'].value_counts()

plt.figure(figsize=(8, 6))
plt.pie(employment_status_counts, labels=employment_status_counts.index, autopct='%1.1f%%', startangle=45)
plt.title('Employment Status for NaN Job Titles')
plt.axis('equal')
plt.show()
```
<br />
It's all around the place. Gathering the information we have so far:

  - Job title has 9027 NaN values (roughly 30%)
  - Missing job titles seem to be all over the place compared their employment status, MCAR (Missing Completely at Random)
  - Imputing would be complicated (but not impossible)
  - Encoding this feature would increase the dimensionality significantly

Based on that, I will remove the feature altogether, and apply OHE (One Hot Encoding).

```{python}
train_df_step3 = train_df_step2.drop(['Job title'], axis=1)

train_df_step4 = pd.get_dummies(train_df_step3, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)
```
<br />

### Correlation matrix
Finally, let's take a look at the correlation matrix.

```{python}
#| label: fig-correlation-matrix
#| fig-cap: Correlation matrix
#| fig-align: center
correlation_matrix = train_df_step4.corr()

plt.figure(figsize=(25,25))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation matrix')
plt.show()
```
<br />
There are two things that might be interesting to explore further: 

  - Correlation between 'Family member count' and 'Children count'.
  - Correlation between 'Employment status_Pensioner' and 'Employment length'.

'Family member count' and 'Children count' are obviously correlated, but it is not a correlation of 1. That means that sometimes, more family members taken into account other than children, or even sometimes children are not taken into account for family, therefore we can't really remove this feature. It is adding some information after all, see @fig-family-children-count.

```{python}
#| label: fig-family-children-count
#| fig-cap: Correlation matrix
#| fig-align: center
family_member_count = train_df_step4['Family member count']
children_count = train_df_step4['Children count']

plt.figure(figsize=(8, 6))
plt.scatter(family_member_count, children_count, alpha=0.5)
plt.title('Scatter Plot of Family Member Count vs Children Count')
plt.xlabel('Family Member Count')
plt.ylabel('Children Count')
plt.grid(True)

plt.gca().xaxis.set_major_locator(MultipleLocator(1))
plt.gca().yaxis.set_major_locator(MultipleLocator(1))

plt.show()
```
<br />
We can see that most of the time, children make up for most of the family members, but sometimes (especifically for 1 family member), there are a few cases with more children than family members.

Moving to the next point to explore, let's take a look at those with an employment status of pensioner and their respective 'Employment length'.

```{python}
employment_status_pensioner = train_df_step4[(train_df_step4['Employment status_Pensioner'] == 1)]
employment_status_pensioner['Employment length'].value_counts()
```
<br />
Out of 4920 pensioners, only 12 have an 'Employment length' different than the value '365243'. That is a 0.24% over the pensioners and 0.04% over the dataset. It is most likely safe to remove one of these two features, as they are explaining the same thing. But which one to remove matters: 'Employment status_Pensioner' has the same information as 'Employment length', but not the other way around. 'Employment length' has information about those that are not Pensioners.

That means we can only remove the Employment status_Pensioner feature.

```{python}
train_df_step5 = train_df_step4.drop(['Employment status_Pensioner'], axis=1)
```
<br />

### Scaling features
At last, before training a machine learning model, we should scale features, given that some of them like 'Income' and 'Employment length' are not proportional to the others.

```{python}
scaler = MinMaxScaler()
train_df_step6 = scaler.fit_transform(train_df_step5)
train_df_step6 = pd.DataFrame(train_df_step6, columns=train_df_step5.columns)
```
<br />

## Training a model
To train a model we first split the data in input (x_data) and output (y_data). Then we apply the following: 
  - Split dataset between train and test sets.
  - Apply cross validation
  - Search for best hyperparameters
  - Make predictions
  - Calculate accuracy

To visually see the results, we can use a confusion matrix.

```{python}
#| label: fig-confusion-matrix-1
#| fig-cap: First model confusion matrix.
#| fig-align: center
y_data = train_df_step6['Is high risk']
x_data = train_df_step6.drop(columns=['Is high risk'])

def train_model(x, y):
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
    rf_classifier = RandomForestClassifier(random_state=42)
    param_grid = {
        'n_estimators': [10, 100],
        'max_depth': [None, 5, 10],
        'min_samples_split': [2, 4],
        'min_samples_leaf': [1, 3]
    }

    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)

    best_params = grid_search.best_params_
    print("Best hyperparameters:", best_params)
    best_model = grid_search.best_estimator_

    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("Accuracy on testing set:", accuracy)
    
    y_pred = best_model.predict(X_test)

    cm = confusion_matrix(y_test, y_pred)

    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()
    
    return best_model
    
best_model = train_model(x_data, y_data)
```
<br />
98.52% accuracy is great! It means almost every observation we made about the dataset were appropiate and helped us build an accurate model... or not.

Some machine learning models, especially those that are tree based, tend to fall into a trap of unbalanced classes. And in this dataset, there's a huge unbalance in our output 'Is high risk'.

```{python}
train_df_step6['Is high risk'].value_counts()
```
<br />
Only 1.74% are high risk. What does that mean? Well... instead of analyzing the dataset, evaluating models, hyperparameters, cross validation and whatnot, it would be more effective to say "No client is high risk". That would give us an accuracy of 98.26%. Looking at the confusion matrix, that is exactly what the model learned and did.
<br />

### Balancing classes
One way to fix this is to balance the classes, either by undersampling or oversampling. Let's try and use oversampling, as we don't really want to loose 28000 entries of valuable information.

```{python}
smote = SMOTE()

x_smote, y_smote = smote.fit_resample(x_data, y_data)

print('Original dataset shape')
print(y_data.value_counts())
print('\nResample dataset shape')
print(y_smote.value_counts())
```
<br />
Now classes are balanced, let's train a new model... a balanced one.

```{python}
#| label: fig-confusion-matrix-2
#| fig-cap: New model confusion matrix.
#| fig-align: center
best_model_balanced = train_model(x_smote, y_smote)
```
<br />

### Comparing balanced and unbalanced models
And now, after balancing our classes, we get an even better accuracy, over 99%! But it wouldn't be fair to compare when we artificially created almost half the data. Let's compare both models, the balanced and unbalanced models, with the entire dataset as input.

```{python}
#| label: fig-confusion-matrix-3
#| fig-cap: Balanced model confusion matrix.
#| fig-align: center
y_pred_balanced = best_model_balanced.predict(x_data)
balanced_accuracy = accuracy_score(y_data, y_pred_balanced)

cm = confusion_matrix(y_data, y_pred_balanced)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Balanced Confusion Matrix')
plt.show()
```
<br />

```{python}
#| label: fig-confusion-matrix-4
#| fig-cap: Unbalanced model confusion matrix.
#| fig-align: center
y_pred_unbalanced = best_model.predict(x_data)
unbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)

cm = confusion_matrix(y_data, y_pred_unbalanced)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Unbalanced Confusion Matrix')
plt.show()
```
<br />
Results for the balanced model:

  - It predicted that over 400 cases were high risk, most of them rightfully so.
  - It predicted that almost all cases were not high risk, most of them rightfully so.

Results for the unbalanced mode: 

  - It predicted that close to 0 cases were high risk.
  - It predicted that almost all cases were not high risk.

Only when measured by their overall accuracy does the balanced model truly shine. But only a glimpse into the confusion matrix clearly shows that the balanced model actually learned from the high risk entries, whereas the unbalanced one barely did.

### Comparing with unseen data
In all fairness though, we're measuring them by almost the same data we trained both models on. Let's throw at them data they have never seen.

```{python}
test_df = pd.get_dummies(test_df, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)
test_df = test_df.drop(['ID','Has a mobile phone','Job title','Employment status_Pensioner'], axis=1)
#with pd.option_context("future.no_silent_downcasting", True):
test_df['Gender'] = test_df['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)
test_df['Has a property'] = test_df['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)
test_df['Has a car'] = test_df['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)
scaler = MinMaxScaler()
test_df_columns = test_df.columns
test_df = scaler.fit_transform(test_df)
test_df = pd.DataFrame(test_df, columns = test_df_columns)

y_data = test_df['Is high risk']
x_data = test_df.drop(columns=['Is high risk'])

y_pred_unbalanced = best_model.predict(x_data)
unbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)

y_pred_balanced = best_model_balanced.predict(x_data)
balanced_accuracy = accuracy_score(y_data, y_pred_balanced)

print(f"Accuracy on dataset with balanced training: {balanced_accuracy*100:.2f}%")
print(f"Accuracy on dataset set with unbalanced training: {unbalanced_accuracy*100:.2f}%")
```
<br />
Overall accuracy of the balanced model dropped to a little over 90% and the unbalanced model stayed high, but is it truly accurate? Let's calculate again the confusion matrices.

```{python}
#| label: fig-confusion-matrix-5
#| fig-cap: Balanced model confusion matrix against unseen data.
#| fig-align: center
cm = confusion_matrix(y_data, y_pred_unbalanced)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Unbalanced Confusion Matrix')
plt.show()
```

```{python}
#| label: fig-confusion-matrix-6
#| fig-cap: Unbalanced model confusion matrix against unseen data.
#| fig-align: center
cm = confusion_matrix(y_data, y_pred_balanced)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title('Balanced Confusion Matrix')
plt.show()
```
<br />
It isn't. The unbalanced model predicted almost no high risk clients, whereas the balanced model predicted multiple cases. It did so correctly at a very low accuracy which is not that good, but that is a story for a different day.
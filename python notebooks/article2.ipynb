{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tabulate import tabulate\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis, norm, kstest\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, QuantileTransformer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./train_data.csv\")\n",
    "test_df = pd.read_csv(\"./test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_matrix(df):\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title('Matriz de correlaciones')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_test(data, p_thres = 0.05):\n",
    "    mean, std = norm.fit(data)\n",
    "\n",
    "    if std == 0:\n",
    "        return 'No normal', 1e-8\n",
    "\n",
    "    normal = norm(loc = mean, scale = std)\n",
    "    _, p_value = stats.kstest(data, normal.cdf)\n",
    "\n",
    "    if p_value > p_thres:\n",
    "        normality = \"Normal\"\n",
    "    else:\n",
    "        normality = \"No normal\"\n",
    "\n",
    "    return normality, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_statistics(dataframe):\n",
    "    data = dataframe.copy()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for col in data.columns:\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(data[col]):\n",
    "            data_stats = data[col].describe(percentiles=[0.25, 0.75])\n",
    "            data_stats['skewness'] = skew(data[col])\n",
    "            data_stats['kurtosis'] = kurtosis(data[col])\n",
    "            data_stats['normalness'], _ = norm_test(data[col], 0.05)\n",
    "            data_stats['uniques'] = data[col].nunique()\n",
    "            data_stats['null count'] = data[col].isnull().sum()\n",
    "            data_stats['data type'] = type(data[col][0])\n",
    "            results[col] = data_stats\n",
    "        else:\n",
    "            results[col] = {'count': data[col].count(), 'unique': data[col].nunique(), 'top': data[col].mode().iloc[0]}\n",
    "\n",
    "    \n",
    "    print(tabulate(pd.DataFrame(results), headers='keys', tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(data, type):\n",
    "    num_plots = min(16, len(data.columns))\n",
    "    num_cols = 4\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, col in enumerate(data.select_dtypes(include=[np.number])): \n",
    "        if i >= num_plots:\n",
    "            break\n",
    "        if type == 'QQPlot': \n",
    "            sm.qqplot(data[col], line='r', ax=axs[i], fit=True)\n",
    "            axs[i].set_xlabel(\"Theoretical Quantiles\")\n",
    "            axs[i].set_ylabel(\"Sample Quantiles\")\n",
    "            axs[i].set_title(f\"{col}\")\n",
    "        elif type == 'BoxPlot':\n",
    "            sns.boxplot(y=data[col], orient=\"v\", ax=axs[i])  \n",
    "            axs[i].set_xlabel(\"\")\n",
    "            axs[i].set_ylabel(col)\n",
    "            axs[i].set_title(f\"Boxplot de {col}\")\n",
    "\n",
    "    \n",
    "    for j in range(num_plots, num_rows * num_cols):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_missing_data(data, method, testing=False, percentage=0.15):\n",
    "    \n",
    "    modified_data = data.copy()\n",
    "\n",
    "    \n",
    "    if testing:\n",
    "        \n",
    "        for column in data:\n",
    "            \n",
    "            indices_to_null = np.random.choice(data.index, size=int(len(data) * percentage), replace=False)\n",
    "            \n",
    "            modified_data.loc[indices_to_null, column] = np.nan\n",
    "\n",
    "    \n",
    "    if method in ['Mean', 'Median', 'Mode', 'Arbitrary']:\n",
    "        \n",
    "        for column in data:\n",
    "            \n",
    "            for index in modified_data[modified_data[column].isnull()].index:\n",
    "                \n",
    "                if method == 'Mean':\n",
    "                    \n",
    "                    value = data[column].mean()\n",
    "                elif method == 'Median':\n",
    "                    \n",
    "                    value = data[column].median()\n",
    "                elif method == 'Mode':\n",
    "                    \n",
    "                    value = data[column].mode().iloc[0]\n",
    "                elif method == 'Arbitrary':\n",
    "                    \n",
    "                    value = np.random.uniform(data[column].min(), data[column].max())\n",
    "                else:\n",
    "                    \n",
    "                    value = np.nan\n",
    "\n",
    "                \n",
    "                if isinstance(value, np.ndarray):\n",
    "                    value = value[0]\n",
    "                \n",
    "                value = data[column].dtype.type(value)\n",
    "\n",
    "                \n",
    "                modified_data.loc[index, column] = value\n",
    "\n",
    "    \n",
    "    elif method in ['KNN', 'MICE']: \n",
    "        if method == 'KNN':\n",
    "            \n",
    "            imputer = KNNImputer(n_neighbors=5) \n",
    "            \n",
    "            imputed = imputer.fit_transform(modified_data)\n",
    "            \n",
    "            modified_data = pd.DataFrame(imputed, columns=modified_data.columns)\n",
    "        elif method == 'MICE':\n",
    "            \n",
    "            imputer = IterativeImputer()\n",
    "            \n",
    "            imputed = imputer.fit_transform(modified_data) \n",
    "            \n",
    "            modified_data = pd.DataFrame(imputed, columns=modified_data.columns)\n",
    "\n",
    "    \n",
    "    return modified_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def handle_outliers(data, method, imputation_method='KNN', winsorization_rate=0.05):\n",
    "    \n",
    "    original_data = data.copy()\n",
    "    \n",
    "    modified_data = data.copy()\n",
    "\n",
    "    if method == 'Imputacion':\n",
    "        for column in modified_data.columns:\n",
    "          \n",
    "            q1 = np.percentile(modified_data[column], 25)\n",
    "            q3 = np.percentile(modified_data[column], 75)\n",
    "            iqr = q3 - q1\n",
    "           \n",
    "            lower_limit = q1 - 1.5 * iqr\n",
    "            upper_limit = q3 + 1.5 * iqr\n",
    "          \n",
    "            modified_data[column] = np.where(modified_data[column] < lower_limit, np.nan, np.where(modified_data[column] > upper_limit, np.nan, modified_data[column]))\n",
    "\n",
    "        \n",
    "        outlierless_data = compute_missing_data(modified_data, imputation_method, False, None)\n",
    "        return outlierless_data\n",
    "    \n",
    "    for column in modified_data.columns:\n",
    "    \n",
    "        q1 = np.percentile(original_data[column], 25)\n",
    "        q3 = np.percentile(original_data[column], 75)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        lower_limit = q1 - 1.5 * iqr\n",
    "        upper_limit = q3 + 1.5 * iqr\n",
    "\n",
    "        if method == 'Trimming':\n",
    "            \n",
    "            mask = (modified_data[column] >= lower_limit) & (modified_data[column] <= upper_limit) \n",
    "            modified_data = modified_data[mask].reset_index(drop=True) \n",
    "            outlierless_data = modified_data.loc[mask] \n",
    "        elif method == 'Capping':\n",
    "            \n",
    "            modified_data[column] = np.where(modified_data[column] < lower_limit, lower_limit, np.where(modified_data[column] > upper_limit, upper_limit, modified_data[column]))\n",
    "            outlierless_data = modified_data \n",
    "        elif method == 'Winsorization':\n",
    "            \n",
    "            lower_winsor = np.percentile(modified_data[column], 100 * winsorization_rate)\n",
    "            upper_winsor = np.percentile(modified_data[column], 100 * (1 - winsorization_rate))\n",
    "            modified_data[column] = np.where(modified_data[column] < lower_winsor, lower_winsor, np.where(modified_data[column] > upper_winsor, upper_winsor, modified_data[column]))\n",
    "            outlierless_data = modified_data\n",
    "        else:\n",
    "            print(f\"No hubo match {method}\")\n",
    "\n",
    "    return outlierless_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, method, p_thres):\n",
    "    modified_data = data.copy()\n",
    "    method_used = []\n",
    "\n",
    "    if method == 'Auto':  \n",
    "        for column in modified_data.columns:\n",
    "            methods = ['Exp', 'BoxCox', 'Yeo-Johnson']  \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            exp_data = np.exp(modified_data[column])\n",
    "            _, p_val_exp = norm_test(exp_data, p_thres = p_thres)\n",
    "\n",
    "            print(exp_data)\n",
    "\n",
    "            \n",
    "            pt = PowerTransformer(method = 'box-cox')\n",
    "            if (modified_data[column] <= 0.0).any():\n",
    "                \n",
    "                box_data = modified_data[column]\n",
    "                p_val_box = 1e-20\n",
    "            else:\n",
    "                box_data = pt.fit_transform(modified_data[column].values.reshape(-1, 1))\n",
    "                _, p_val_box = norm_test(box_data, p_thres = p_thres)\n",
    "\n",
    "            print(box_data)\n",
    "\n",
    "            \n",
    "            pt = PowerTransformer(method = 'yeo-johnson')\n",
    "            yeo_data = pt.fit_transform(modified_data[column].values.reshape(-1, 1))\n",
    "            _, p_val_yeo = norm_test(yeo_data, p_thres = p_thres)\n",
    "\n",
    "            print(yeo_data)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            data = [exp_data, box_data, yeo_data]\n",
    "\n",
    "            print(data)\n",
    "            \n",
    "            p_vals = [p_val_exp, p_val_box, p_val_yeo] \n",
    "            index = np.argmax(p_vals)  \n",
    "            modified_data[column] = data[index]  \n",
    "            method_used.append(methods[index]) \n",
    "        return modified_data, method_used\n",
    "    else:\n",
    "        for column in modified_data.columns:\n",
    "            \n",
    "            if norm_test(modified_data[column], p_thres = p_thres) == 'Normal':\n",
    "                continue\n",
    "\n",
    "            if method == 'Exp':\n",
    "                if (modified_data[column] > 20).any(): \n",
    "                    print('Los numeros son demasiado grandes para hacer transformacion exponencial')\n",
    "                    method_used.append('none') \n",
    "                    continue\n",
    "                modified_data[column] = np.exp(modified_data[column]) \n",
    "                method_used.append('Exp')  \n",
    "            elif method == 'box-cox':\n",
    "                if (modified_data[column] <= 0.0).any():  \n",
    "                    print('Hay numeros negativos o cero, no se puede hacer box-cox')\n",
    "                    method_used.append('none')\n",
    "                    continue\n",
    "                pt = PowerTransformer(method = method) \n",
    "                modified_data[column] = pt.fit_transform(modified_data[column].values.reshape(-1, 1)) \n",
    "                method_used.append('box-cox')\n",
    "            elif method == 'yeo-johnson':\n",
    "                pt = PowerTransformer(method = method)\n",
    "                modified_data[column] = pt.fit_transform(modified_data[column].values.reshape(-1, 1)) \n",
    "                method_used.append('Yeo-Johnson')\n",
    "            elif method == 'Cuartiles':\n",
    "                qt = QuantileTransformer(n_quantiles = 100, output_distribution='normal')\n",
    "                modified_data[column] = qt.fit_transform(modified_data[[column]]) \n",
    "                method_used.append('Cuartiles')\n",
    "            else:\n",
    "                print(f\"No hubo match {method}\")\n",
    "    return modified_data, method_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(data, method, corr_thres = 0.95, var_thres = 0.01, normality_thres = 0.01, explained_var = 0.99, do_ica = False, filter_non_normal = False):\n",
    "    modified_data = data.copy()\n",
    "\n",
    "    if method == 'Filter':\n",
    "        \n",
    "        for column in modified_data.columns:  \n",
    "            var = modified_data[column].var() \n",
    "\n",
    "            _, p_val = norm_test(modified_data[column], normality_thres) \n",
    "\n",
    "            if var < var_thres:  \n",
    "                print(f'Quitando {column} por varianza {var}')  \n",
    "                modified_data.drop(column, axis=1, inplace=True)  \n",
    "            elif p_val < normality_thres and filter_non_normal:  \n",
    "                print(f'Quitando {column} por no normal {p_val}')  \n",
    "                modified_data.drop(column, axis=1, inplace=True)  \n",
    "\n",
    "        \n",
    "        corr_matrix = np.corrcoef(modified_data, rowvar=False)\n",
    "        \n",
    "        np.fill_diagonal(corr_matrix, 0)\n",
    "        corr_mask = np.abs(corr_matrix) < corr_thres \n",
    "        modified_data = modified_data.loc[:, np.all(corr_mask, axis=0)] \n",
    "\n",
    "    elif method == 'Projection': \n",
    "        scaler = StandardScaler()   \n",
    "        modified_data = scaler.fit_transform(modified_data) \n",
    "\n",
    "        \n",
    "        pca = PCA(explained_var) \n",
    "        modified_data = pca.fit_transform(modified_data)\n",
    "        print(f'Explained variance ratio: {pca.explained_variance_ratio_}')\n",
    "        \n",
    "        if pca.n_components_ > 1 and do_ica:\n",
    "            ica = FastICA()\n",
    "            modified_data = ica.fit_transform(modified_data) \n",
    "\n",
    "        modified_data = pd.DataFrame(modified_data, columns=[f\"PC{i}\" for i in range(1, pca.n_components_ + 1)]) \n",
    "\n",
    "    else:\n",
    "        print(f\"No hubo match {method}\")\n",
    "\n",
    "    return modified_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero revisamos el dataset. Antes de cualquier manipulacion de los datos, debemos explorar, entender los datos y finalmente comprender lo que deseamos hacer con los mismos. En este caso lo que se intenta es determinar si una persona es elegible para una tarjeta de credito. La variable de interes para este caso es \"Is High Risk\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Has a car</th>\n",
       "      <th>Has a property</th>\n",
       "      <th>Children count</th>\n",
       "      <th>Income</th>\n",
       "      <th>Employment status</th>\n",
       "      <th>Education level</th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Dwelling</th>\n",
       "      <th>Age</th>\n",
       "      <th>Employment length</th>\n",
       "      <th>Has a mobile phone</th>\n",
       "      <th>Has a work phone</th>\n",
       "      <th>Has a phone</th>\n",
       "      <th>Has an email</th>\n",
       "      <th>Job title</th>\n",
       "      <th>Family member count</th>\n",
       "      <th>Account age</th>\n",
       "      <th>Is high risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5037048</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>With parents</td>\n",
       "      <td>-16271</td>\n",
       "      <td>-3111</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5044630</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-10130</td>\n",
       "      <td>-1651</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Accountants</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5079079</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-12821</td>\n",
       "      <td>-5657</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Laborers</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5112872</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>Commercial associate</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-20929</td>\n",
       "      <td>-2046</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Managers</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5105858</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Separated</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-16207</td>\n",
       "      <td>-515</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID Gender Has a car Has a property  Children count    Income  \\\n",
       "0  5037048      M         Y              Y               0  135000.0   \n",
       "1  5044630      F         Y              N               1  135000.0   \n",
       "2  5079079      F         N              Y               2  180000.0   \n",
       "3  5112872      F         Y              Y               0  360000.0   \n",
       "4  5105858      F         N              N               0  270000.0   \n",
       "\n",
       "      Employment status                Education level        Marital status  \\\n",
       "0               Working  Secondary / secondary special               Married   \n",
       "1  Commercial associate               Higher education  Single / not married   \n",
       "2  Commercial associate  Secondary / secondary special               Married   \n",
       "3  Commercial associate               Higher education  Single / not married   \n",
       "4               Working  Secondary / secondary special             Separated   \n",
       "\n",
       "            Dwelling    Age  Employment length  Has a mobile phone  \\\n",
       "0       With parents -16271              -3111                   1   \n",
       "1  House / apartment -10130              -1651                   1   \n",
       "2  House / apartment -12821              -5657                   1   \n",
       "3  House / apartment -20929              -2046                   1   \n",
       "4  House / apartment -16207               -515                   1   \n",
       "\n",
       "   Has a work phone  Has a phone  Has an email    Job title  \\\n",
       "0                 0            0             0   Core staff   \n",
       "1                 0            0             0  Accountants   \n",
       "2                 0            0             0     Laborers   \n",
       "3                 0            0             1     Managers   \n",
       "4                 0            1             0          NaN   \n",
       "\n",
       "   Family member count  Account age  Is high risk  \n",
       "0                  2.0        -17.0             0  \n",
       "1                  2.0         -1.0             0  \n",
       "2                  4.0        -38.0             0  \n",
       "3                  1.0        -11.0             0  \n",
       "4                  1.0        -41.0             0  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer analisis exploratorio a realizar es si hay alguna variable que no aporta significancia al resultado. En este dataset podemos ver que el ID de un cliente no ayuda a determinar si una persona es o no elegible para tarjeta de credito. Es simplemente un valor que el banco les asignó para identificarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_step1 = train_df.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorando mas a fondo, vemos que todas las personas del dataset tienen telefono movil, por lo que tampoco aporta informacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(train_df_step1['Has a mobile phone'].unique())\n",
    "train_df_step2 = train_df_step1.drop(['Has a mobile phone'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora nos enfocamos en el tipo de variables que hay: \n",
    "- Categoricas\n",
    "    - Nominales\n",
    "    - Ordinales\n",
    "- Numericas\n",
    "    - Discretas\n",
    "    - Continuas\n",
    "\n",
    "Las numericas no requieren transformacion, pero las categoricas si se deben convertir en numericas para poder analizarlas con cualquier modelo. Las mas sencillas son Gender, Has car y Has a property, ya que se remplazan directamente por 1 y 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"future.no_silent_downcasting\", True): #hacemos esto porque remplazar toda una columna de string por numeros enteros levanta sospechas a pandas\n",
    "    train_df_step2['Gender'] = train_df_step2['Gender'].replace({'M': 0, 'F': 1})\n",
    "    train_df_step2['Has a car'] = train_df_step2['Has a car'].replace({'N':0, 'Y':1})\n",
    "    train_df_step2['Has a property'] = train_df_step2['Has a car'].replace({'N':0, 'Y':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employment status, Education level, Marital status, Dwelling y Job title debemos analizarlas con mas detalle para ver cuantas categorias contiene cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employment status: ['Working' 'Commercial associate' 'Pensioner' 'State servant' 'Student']\n",
      "\n",
      "Education level: ['Secondary / secondary special' 'Higher education' 'Lower secondary'\n",
      " 'Incomplete higher' 'Academic degree']\n",
      "\n",
      "Marital status: ['Married' 'Single / not married' 'Separated' 'Civil marriage' 'Widow']\n",
      "\n",
      "Dwelling: ['With parents' 'House / apartment' 'Municipal apartment'\n",
      " 'Rented apartment' 'Office apartment' 'Co-op apartment']\n",
      "\n",
      "Job title: ['Core staff' 'Accountants' 'Laborers' 'Managers' nan 'Sales staff'\n",
      " 'Medicine staff' 'High skill tech staff' 'HR staff' 'Low-skill Laborers'\n",
      " 'Drivers' 'Secretaries' 'Cleaning staff' 'Cooking staff' 'Security staff'\n",
      " 'Private service staff' 'IT staff' 'Waiters/barmen staff' 'Realty agents']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Employment status: {train_df['Employment status'].unique()}\\n\")\n",
    "print(f\"Education level: {train_df['Education level'].unique()}\\n\")\n",
    "print(f\"Marital status: {train_df['Marital status'].unique()}\\n\")\n",
    "print(f\"Dwelling: {train_df['Dwelling'].unique()}\\n\")\n",
    "print(f\"Job title: {train_df['Job title'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employment status, Marital status, Dwelling y Job title son variables nominales mientras que Education level puede considerarse ordinal. \n",
    "\n",
    "A las variables nominales les podriamos aplicar One Hot Encoding, pero entre todas ellas tenemos 35 valores, por lo que terminariamos con muchas mas dimensiones que variables iniciales. Podriamos utilizar Binary Encoding, pero exploremos un poco mas los datos. Una observacion importante es que Job title tiene valores nulos, pero siempre se debe analizar con cuidado los datos. Uno podria suponer que aquellas personas sin titulo de trabajo, estan desempleadas, pero no debemos suponer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employment status: ['Working' 'Pensioner' 'Commercial associate' 'State servant']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nan_job_df = train_df[train_df['Job title'].isna()]\n",
    "print(f\"Employment status: {nan_job_df['Employment status'].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que efectivamente algunas personas sin Job title asignado, tienen Employment status que indica que si tienen trabajo. En este caso, el titulo del trabajo de una persona no nos brinda tanta informacion sobre su eligibilidad para recibir una tarjeta de credito (principalmente porque tenemos la informacion completa acerca de sus ingresos). Pero si se quisiera utilizar esta informacion, se podria hacer imputacion de datos (algun algoritmo de clusterizacion como KNN, asumir la moda o incluso entrenar un modelo para primero rellenar esos valores). Quitando Job title, podemos utilizar One Hot Encoding sin agregar tantas dimensiones al dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train_df_step3 \u001b[38;5;241m=\u001b[39m train_df_step2\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob title\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m train_df_step4 \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(train_df_step3[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmployment status\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEducation level\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMarital status\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDwelling\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "\u001b[1;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "source": [
    "train_df_step3 = train_df_step2.drop(['Job title'], axis=1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "train_df_step4 = encoder.fit_transform(train_df_step3[['Employment status', 'Education level', 'Marital status', 'Dwelling']])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

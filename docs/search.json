[
  {
    "objectID": "python notebooks/Exploring a dataset.html",
    "href": "python notebooks/Exploring a dataset.html",
    "title": "Journey into Data Science",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MultipleLocator\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\n\n\ntrain_df = pd.read_csv(\"./train_data.csv\")\ntest_df = pd.read_csv(\"./test_data.csv\")\n\nPrimero revisamos el dataset. Antes de cualquier manipulacion de los datos, debemos explorar, entender los datos y finalmente comprender lo que deseamos hacer con los mismos. En este caso lo que se intenta es determinar si una persona es elegible para una tarjeta de credito. La variable de interes para este caso es “Is High Risk”.\n\ntrain_df.head(5)\n\n\n\n\n\n\n\n\n\nID\nGender\nHas a car\nHas a property\nChildren count\nIncome\nEmployment status\nEducation level\nMarital status\nDwelling\nAge\nEmployment length\nHas a mobile phone\nHas a work phone\nHas a phone\nHas an email\nJob title\nFamily member count\nAccount age\nIs high risk\n\n\n\n\n0\n5037048\nM\nY\nY\n0\n135000.0\nWorking\nSecondary / secondary special\nMarried\nWith parents\n-16271\n-3111\n1\n0\n0\n0\nCore staff\n2.0\n-17.0\n0\n\n\n1\n5044630\nF\nY\nN\n1\n135000.0\nCommercial associate\nHigher education\nSingle / not married\nHouse / apartment\n-10130\n-1651\n1\n0\n0\n0\nAccountants\n2.0\n-1.0\n0\n\n\n2\n5079079\nF\nN\nY\n2\n180000.0\nCommercial associate\nSecondary / secondary special\nMarried\nHouse / apartment\n-12821\n-5657\n1\n0\n0\n0\nLaborers\n4.0\n-38.0\n0\n\n\n3\n5112872\nF\nY\nY\n0\n360000.0\nCommercial associate\nHigher education\nSingle / not married\nHouse / apartment\n-20929\n-2046\n1\n0\n0\n1\nManagers\n1.0\n-11.0\n0\n\n\n4\n5105858\nF\nN\nN\n0\n270000.0\nWorking\nSecondary / secondary special\nSeparated\nHouse / apartment\n-16207\n-515\n1\n0\n1\n0\nNaN\n1.0\n-41.0\n0\n\n\n\n\n\n\n\n\n\ntrain_df.describe()\n\n\n\n\n\n\n\n\n\nID\nChildren count\nIncome\nAge\nEmployment length\nHas a mobile phone\nHas a work phone\nHas a phone\nHas an email\nFamily member count\nAccount age\nIs high risk\n\n\n\n\ncount\n2.916500e+04\n29165.000000\n2.916500e+04\n29165.000000\n29165.000000\n29165.0\n29165.000000\n29165.000000\n29165.000000\n29165.000000\n29165.000000\n29165.000000\n\n\nmean\n5.078232e+06\n0.430790\n1.868904e+05\n-15979.477490\n59257.761255\n1.0\n0.224310\n0.294977\n0.090279\n2.197531\n-26.137734\n0.017110\n\n\nstd\n4.182400e+04\n0.741882\n1.014096e+05\n4202.997485\n137655.883458\n0.0\n0.417134\n0.456040\n0.286587\n0.912189\n16.486702\n0.129682\n\n\nmin\n5.008804e+06\n0.000000\n2.700000e+04\n-25152.000000\n-15713.000000\n1.0\n0.000000\n0.000000\n0.000000\n1.000000\n-60.000000\n0.000000\n\n\n25%\n5.042047e+06\n0.000000\n1.215000e+05\n-19444.000000\n-3153.000000\n1.0\n0.000000\n0.000000\n0.000000\n2.000000\n-39.000000\n0.000000\n\n\n50%\n5.074666e+06\n0.000000\n1.575000e+05\n-15565.000000\n-1557.000000\n1.0\n0.000000\n0.000000\n0.000000\n2.000000\n-24.000000\n0.000000\n\n\n75%\n5.114629e+06\n1.000000\n2.250000e+05\n-12475.000000\n-412.000000\n1.0\n0.000000\n1.000000\n0.000000\n3.000000\n-12.000000\n0.000000\n\n\nmax\n5.150485e+06\n19.000000\n1.575000e+06\n-7705.000000\n365243.000000\n1.0\n1.000000\n1.000000\n1.000000\n20.000000\n0.000000\n1.000000\n\n\n\n\n\n\n\n\nEl primer analisis exploratorio a realizar es si hay alguna variable que no aporta significancia al resultado. En este dataset podemos ver que el ID de un cliente no ayuda a determinar si una persona es o no elegible para tarjeta de credito. Es simplemente un valor que el banco les asignó para identificarlos.\n\ntrain_df_step1 = train_df.drop(['ID'], axis=1)\n\nExplorando mas a fondo, vemos que todas las personas del dataset tienen telefono movil, por lo que tampoco aporta informacion.\n\ntrain_df_step2 = train_df_step1.drop(['Has a mobile phone'], axis=1)\n\nAhora nos enfocamos en el tipo de variables que hay: - Categoricas - Nominales - Ordinales - Numericas - Discretas - Continuas\nLas numericas no requieren transformacion, pero las categoricas si se deben convertir en numericas para poder analizarlas con cualquier modelo. Las mas sencillas son Gender, Has car y Has a property, ya que se remplazan directamente por 1 y 0.\n\n#replacing an entire column of strings for integers raises some warnings\nwith pd.option_context(\"future.no_silent_downcasting\", True): \n    train_df_step2['Gender'] = train_df_step2['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)\n    train_df_step2['Has a car'] = train_df_step2['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)\n    train_df_step2['Has a property'] = train_df_step2['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)\n\nEmployment status, Education level, Marital status, Dwelling y Job title debemos analizarlas con mas detalle para ver cuantas categorias contiene cada una.\n\n# List of features to visualize\nfeatures = ['Employment status', 'Education level', 'Marital status', 'Dwelling', 'Job title']\n\n# Loop through the features and create a count plot for each\nfor feature in features:\n    plt.figure(figsize=(10, 6))\n    sns.countplot(data=train_df_step2, x=feature)\n    plt.title(f'Count of Unique Values in {feature}')\n    plt.xlabel('')\n    plt.xticks(rotation=90)\n    plt.ylabel('Counts')\n    \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmployment status, Marital status, Dwelling y Job title son variables nominales mientras que Education level puede considerarse ordinal.\nA las variables nominales les podriamos aplicar One Hot Encoding, pero entre todas ellas tenemos 35 valores, por lo que terminariamos con muchas mas dimensiones que variables iniciales. Podriamos utilizar Binary Encoding, pero exploremos un poco mas los datos. Una observacion importante es que Job title tiene valores nulos, pero siempre se debe analizar con cuidado los datos. Uno podria suponer que aquellas personas sin titulo de trabajo, estan desempleadas, pero no debemos suponer.\n\ntrain_df.isna().sum()\n\nID                        0\nGender                    0\nHas a car                 0\nHas a property            0\nChildren count            0\nIncome                    0\nEmployment status         0\nEducation level           0\nMarital status            0\nDwelling                  0\nAge                       0\nEmployment length         0\nHas a mobile phone        0\nHas a work phone          0\nHas a phone               0\nHas an email              0\nJob title              9027\nFamily member count       0\nAccount age               0\nIs high risk              0\ndtype: int64\n\n\n\nnan_job_title_df = train_df[train_df['Job title'].isna()]\nemployment_status_counts = nan_job_title_df['Employment status'].value_counts()\n\nplt.figure(figsize=(8, 6))\nplt.pie(employment_status_counts, labels=employment_status_counts.index, autopct='%1.1f%%', startangle=45)\nplt.title('Employment Status for NaN Job Titles')\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\nPodemos ver que efectivamente algunas personas sin Job title asignado, tienen Employment status que indica que si tienen trabajo. En este caso, el titulo del trabajo de una persona no nos brinda tanta informacion sobre su eligibilidad para recibir una tarjeta de credito (principalmente porque tenemos la informacion completa acerca de sus ingresos). Pero si se quisiera utilizar esta informacion, se podria hacer imputacion de datos (algun algoritmo de clusterizacion como KNN, asumir la moda o incluso entrenar un modelo para primero rellenar esos valores). Quitando Job title, podemos utilizar One Hot Encoding sin agregar tantas dimensiones al dataset.\n\ntrain_df_step3 = train_df_step2.drop(['Job title'], axis=1)\n\ntrain_df_step4 = pd.get_dummies(train_df_step3, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)\n\nA continuacion vamos a revisar la matriz de correlacion.\n\ncorrelation_matrix = train_df_step4.corr()\n\nplt.figure(figsize=(25,25))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation matrix')\nplt.show()\n\n\n\n\n\n\n\n\nSe pueden ver varios puntos importantes a explorar: 1. La correlacion de 1 entre ‘employment status_pensioner’ y ‘employment length’ 2. La alta correlacion entre ‘family member count’ y ‘children count’\nEn cuanto a ‘family member count’ y ‘children count’, la correlatividad no es de 1 pero muy alta. Esto es de esperarse ya que la cantidad de miembros de familia esta muy relacionada a la cantidad de hijos, pero una variable toma en cuenta otro tipo de familiares. Como una variable aporta algo de informacion adicional, no podemos eliminar ninguna de ellas.\n\nfamily_member_count = train_df_step4['Family member count']\nchildren_count = train_df_step4['Children count']\n\nplt.figure(figsize=(8, 6))\nplt.scatter(family_member_count, children_count, alpha=0.5)\nplt.title('Scatter Plot of Family Member Count vs Children Count')\nplt.xlabel('Family Member Count')\nplt.ylabel('Children Count')\nplt.grid(True)\n\nplt.gca().xaxis.set_major_locator(MultipleLocator(1))\nplt.gca().yaxis.set_major_locator(MultipleLocator(1))\n\nplt.show()\n\n\n\n\n\n\n\n\nDentro de los 4908 pensionados, solo 12 tienen valor distinto a 365243 (un 0.24%). Y sobre el dataset entero, el porcentaje baja a 0.04%. En este caso se puede eliminar una de las variables, pero a diferencia de los otros casos, aqui si importa cual de las dos se elimina. Si eliminasemos employment length, se perderia toda la informacion de la gente que si tiene empleo, por lo que solamente podemos eliminar la columna de pensionados, ya que toda su informacion esta contenida en ‘employment length’.\n\nemployment_status_pensioner = train_df_step4[(train_df_step4['Employment status_Pensioner'] == 1)]\nemployment_status_pensioner['Employment length'].value_counts()\n\nEmployment length\n 365243    4908\n-678          1\n-443          1\n-586          1\n-672          1\n-673          1\n-3680         1\n-2745         1\n-2208         1\n-620          1\n-1325         1\n-1644         1\n-2269         1\nName: count, dtype: int64\n\n\n\ntrain_df_step5 = train_df_step4.drop(['Employment status_Pensioner'], axis=1)\n\nLast step should be scaling variables, since some of them like ‘Income’ and ‘Employment length’ are not proportional in scale to the others.\n\nscaler = MinMaxScaler()\ntrain_df_step6 = scaler.fit_transform(train_df_step5)\ntrain_df_step6 = pd.DataFrame(train_df_step6, columns=train_df_step5.columns)\n\n\ny_data = train_df_step6['Is high risk']\nx_data = train_df_step6.drop(columns=['Is high risk'])\n\ndef train_model(x, y):\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    \n    rf_classifier = RandomForestClassifier(random_state=42)\n    param_grid = {\n        'n_estimators': [10, 100],\n        'max_depth': [None, 5, 10],\n        'min_samples_split': [2, 4],\n        'min_samples_leaf': [1, 3]\n    }\n\n    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n    grid_search.fit(X_train, y_train)\n\n    best_params = grid_search.best_params_\n    print(\"Best hyperparameters:\", best_params)\n    best_model = grid_search.best_estimator_\n\n    y_pred = best_model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy on testing set:\", accuracy)\n    \n    y_pred = best_model.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.show()\n    \n    return best_model\n    \nbest_model = train_model(x_data, y_data)\n\nBest hyperparameters: {'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100}\nAccuracy on testing set: 0.9852563003600205\n\n\n\n\n\n\n\n\n\n98.52% de precision! Esto significa que todas las observaciones que hicimos acerca del dataset fueron apropiadas y ayudaron a construir un modelo con excelente precision… o no.\nAlgunos modelos de machine learning, especialmente los basados en arboles, tienden a caer en una trampa cuando se trabaja con clases no balanceadas. En este dataset hay un desbalance enorme de ‘Is High Risk’.\n\ntrain_df_step6['Is high risk'].value_counts()\n\nIs high risk\n0.0    28666\n1.0      499\nName: count, dtype: int64\n\n\nSolamente un 1.74% de los clientes son de riesgo. Que significa esto? En vez de hacer un analisis del dataset y evaluar modelos, hiperparametros, hacer crossvalidation, etc., podriamos unicamente predecir que ningun cliente es de riesgo. Esto nos daria una precision de 98.26%, la cual es mejor que el modelo que acabamos de evaluar. Los modelos de machine learning trabajan en su mejor condicion cuando las clases estan balanceadas, asi que nuestro ultimo paso será este balance.\n\nsmote = SMOTE()\n\nx_smote, y_smote = smote.fit_resample(x_data, y_data)\n\nprint('Original dataset shape')\nprint(y_data.value_counts())\nprint('\\nResample dataset shape')\nprint(y_smote.value_counts())\n\nOriginal dataset shape\nIs high risk\n0.0    28666\n1.0      499\nName: count, dtype: int64\n\nResample dataset shape\nIs high risk\n0.0    28666\n1.0    28666\nName: count, dtype: int64\n\n\nEntrenamos nuevamente el modelo pero con las clases balanceadas (28666 casos de riesgo y no riesgo), y obtenemos una precision mayor a la del primer modelo, e incluso mayor a solo predecir que un cliente no es de riesgo. En la matriz de confusion podemos ver que el modelo esta debidamente prediciendo tanto clientes de riesgo como clientes que no son de riesgo.\n\nbest_model_balanced = train_model(x_smote, y_smote)\n\nBest hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 100}\nAccuracy on testing set: 0.9903200488357896\n\n\n\n\n\n\n\n\n\nVeamos la comparacion entre utilizar el modelo entrenado con clases balanceadas, y el modelo entrenado sin clases balanceadas. En el modelo balanceado se predijeron correctamente 388 casos de riesgo, e incorrectamente 111 casos de riesgo (77.76%). En el modelo no balanceado se predijeron correctamente 15 casos de riesgo, e incorrectamente 484 casos de riesgo (3.00%).\n\ny_pred_balanced = best_model_balanced.predict(x_data)\nbalanced_accuracy = accuracy_score(y_data, y_pred_balanced)\n\ncm = confusion_matrix(y_data, y_pred_balanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Balanced Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\ny_pred_unbalanced = best_model.predict(x_data)\nunbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)\n\ncm = confusion_matrix(y_data, y_pred_unbalanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Unbalanced Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\ntest_df = pd.read_csv(\"./test_data.csv\")\ntest_df = pd.get_dummies(test_df, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)\ntest_df = test_df.drop(['ID','Has a mobile phone','Job title','Employment status_Pensioner'], axis=1)\nwith pd.option_context(\"future.no_silent_downcasting\", True):\n    test_df['Gender'] = test_df['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)\n    test_df['Has a property'] = test_df['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)\n    test_df['Has a car'] = test_df['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)\nscaler = MinMaxScaler()\ntest_df_columns = test_df.columns\ntest_df = scaler.fit_transform(test_df)\ntest_df = pd.DataFrame(test_df, columns = test_df_columns)\n\n\ny_data = test_df['Is high risk']\nx_data = test_df.drop(columns=['Is high risk'])\n\ny_pred_unbalanced = best_model.predict(x_data)\nunbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)\n\ny_pred_balanced = best_model_balanced.predict(x_data)\nbalanced_accuracy = accuracy_score(y_data, y_pred_balanced)\n\nprint(f\"Accuracy on dataset with balanced training: {balanced_accuracy*100:.2f}%\")\nprint(f\"Accuracy on dataset set with unbalanced training: {unbalanced_accuracy*100:.2f}%\")\n\nAccuracy on dataset with balanced training: 90.65%\nAccuracy on dataset set with unbalanced training: 98.40%\n\n\n\ncm = confusion_matrix(y_data, y_pred_unbalanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Unbalanced Confusion Matrix')\nplt.show()\n\ncm = confusion_matrix(y_data, y_pred_balanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Balanced Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "articles/002-Datasets.html",
    "href": "articles/002-Datasets.html",
    "title": "Exploring Datasets",
    "section": "",
    "text": "In the world of data science and machine learning, understanding the significance of not just diving headfirst into building models but rather meticulously exploring and understanding the datasets at hand is crucial.\nIn this article I embark on a journey through the pre-modeling phase, focusing on essential exploratory data analysis (EDA) techniques. I am going to be using a dataset from Kaggle, a website where one can find different datasets, people can upload their machine learning models, there’s even competition and sometimes prizes for winners. The dataset is called “Credit Car Prediction”, and it has multiple variables regarding the financial and social status of individuals. The goal of the dataset is to determine if an individual is or not eligible for a credit card.\nWe start off importing the libraries we are going to be using, and loading the dataset.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MultipleLocator\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\n\ntrain_df = pd.read_csv(\"./002-train_data.csv\")\ntest_df = pd.read_csv(\"./002-test_data.csv\")"
  },
  {
    "objectID": "articles/002-Datasets.html#introduction",
    "href": "articles/002-Datasets.html#introduction",
    "title": "Exploring Datasets",
    "section": "",
    "text": "In the world of data science and machine learning, understanding the significance of not just diving headfirst into building models but rather meticulously exploring and understanding the datasets at hand is crucial.\nIn this article I embark on a journey through the pre-modeling phase, focusing on essential exploratory data analysis (EDA) techniques. I am going to be using a dataset from Kaggle, a website where one can find different datasets, people can upload their machine learning models, there’s even competition and sometimes prizes for winners. The dataset is called “Credit Car Prediction”, and it has multiple variables regarding the financial and social status of individuals. The goal of the dataset is to determine if an individual is or not eligible for a credit card.\nWe start off importing the libraries we are going to be using, and loading the dataset.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MultipleLocator\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\n\ntrain_df = pd.read_csv(\"./002-train_data.csv\")\ntest_df = pd.read_csv(\"./002-test_data.csv\")"
  },
  {
    "objectID": "articles/002-Datasets.html#exploring-the-dataset",
    "href": "articles/002-Datasets.html#exploring-the-dataset",
    "title": "Exploring Datasets",
    "section": "Exploring the dataset",
    "text": "Exploring the dataset\nLet’s take a look at the dataset, see what types of features there are, what information they bring to the table, how many there are, how many entries and some of the statistics of each.\n\ntrain_df.head(5)\n\n\n\n\n\n\n\n\n\nID\nGender\nHas a car\nHas a property\nChildren count\nIncome\nEmployment status\nEducation level\nMarital status\nDwelling\nAge\nEmployment length\nHas a mobile phone\nHas a work phone\nHas a phone\nHas an email\nJob title\nFamily member count\nAccount age\nIs high risk\n\n\n\n\n0\n5037048\nM\nY\nY\n0\n135000.0\nWorking\nSecondary / secondary special\nMarried\nWith parents\n-16271\n-3111\n1\n0\n0\n0\nCore staff\n2.0\n-17.0\n0\n\n\n1\n5044630\nF\nY\nN\n1\n135000.0\nCommercial associate\nHigher education\nSingle / not married\nHouse / apartment\n-10130\n-1651\n1\n0\n0\n0\nAccountants\n2.0\n-1.0\n0\n\n\n2\n5079079\nF\nN\nY\n2\n180000.0\nCommercial associate\nSecondary / secondary special\nMarried\nHouse / apartment\n-12821\n-5657\n1\n0\n0\n0\nLaborers\n4.0\n-38.0\n0\n\n\n3\n5112872\nF\nY\nY\n0\n360000.0\nCommercial associate\nHigher education\nSingle / not married\nHouse / apartment\n-20929\n-2046\n1\n0\n0\n1\nManagers\n1.0\n-11.0\n0\n\n\n4\n5105858\nF\nN\nN\n0\n270000.0\nWorking\nSecondary / secondary special\nSeparated\nHouse / apartment\n-16207\n-515\n1\n0\n1\n0\nNaN\n1.0\n-41.0\n0\n\n\n\n\n\n\n\n\n\n\ntrain_df.describe()\n\n\n\n\n\n\n\n\n\nID\nChildren count\nIncome\nAge\nEmployment length\nHas a mobile phone\nHas a work phone\nHas a phone\nHas an email\nFamily member count\nAccount age\nIs high risk\n\n\n\n\ncount\n2.916500e+04\n29165.000000\n2.916500e+04\n29165.000000\n29165.000000\n29165.0\n29165.000000\n29165.000000\n29165.000000\n29165.000000\n29165.000000\n29165.000000\n\n\nmean\n5.078232e+06\n0.430790\n1.868904e+05\n-15979.477490\n59257.761255\n1.0\n0.224310\n0.294977\n0.090279\n2.197531\n-26.137734\n0.017110\n\n\nstd\n4.182400e+04\n0.741882\n1.014096e+05\n4202.997485\n137655.883458\n0.0\n0.417134\n0.456040\n0.286587\n0.912189\n16.486702\n0.129682\n\n\nmin\n5.008804e+06\n0.000000\n2.700000e+04\n-25152.000000\n-15713.000000\n1.0\n0.000000\n0.000000\n0.000000\n1.000000\n-60.000000\n0.000000\n\n\n25%\n5.042047e+06\n0.000000\n1.215000e+05\n-19444.000000\n-3153.000000\n1.0\n0.000000\n0.000000\n0.000000\n2.000000\n-39.000000\n0.000000\n\n\n50%\n5.074666e+06\n0.000000\n1.575000e+05\n-15565.000000\n-1557.000000\n1.0\n0.000000\n0.000000\n0.000000\n2.000000\n-24.000000\n0.000000\n\n\n75%\n5.114629e+06\n1.000000\n2.250000e+05\n-12475.000000\n-412.000000\n1.0\n0.000000\n1.000000\n0.000000\n3.000000\n-12.000000\n0.000000\n\n\nmax\n5.150485e+06\n19.000000\n1.575000e+06\n-7705.000000\n365243.000000\n1.0\n1.000000\n1.000000\n1.000000\n20.000000\n0.000000\n1.000000\n\n\n\n\n\n\n\n\n\nDropping features\nID can be quickly dropped, as it provides no information about the client. It is merely used by databases to assign a unique value to any given client.\n\ntrain_df_step1 = train_df.drop(['ID'], axis=1)\n\n Looking at the mobile phones statistics:\n\nIt has a mean of 1\nIt has a standart deviation of 0\nIt has a minimum of 1\nIt has a maximum of 1\n\nIt is simply a feature full of 1’s, it provides no information. If we encountered in the validation set or real life a person with no mobile phone, the model can’t possibly learn anything about them.\n\ntrain_df_step2 = train_df_step1.drop(['Has a mobile phone'], axis=1)\n\n\n\nTransforming categorical features\nFocusing on categorical features, there are three that we need to transform into discrete features.\n\nGender we replace M to 0 and F to 1.\nHas a car we replace N to 0 and Y to 1.\nHas a property we replace N to 0 and Y to 1.\n\n\n#replacing an entire column of strings for integers raises some warnings\n#with pd.option_context(\"future.no_silent_downcasting\", True): \ntrain_df_step2['Gender'] = train_df_step2['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)\ntrain_df_step2['Has a car'] = train_df_step2['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)\ntrain_df_step2['Has a property'] = train_df_step2['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)\n\n\n\nWarning: Replacing an entire column of strings for integers raises some warnings. Consider using “#with pd.option_context(”future.no_silent_downcasting”, True):” to remove the warning.\n\nNext up are features that have more than two possible categories, like Employment status, Education level, Marital status, Dwelling and Job title.\nPlotting them is a very helpful way to visualize how many categories there are and the population of each. See Figure 5, Figure 5, Figure 5, Figure 5, Figure 5.\nfeatures = ['Employment status', 'Education level', 'Marital status', 'Dwelling', 'Job title']\n\nfor feature in features:\n    plt.figure(figsize=(10, 6))\n    sns.countplot(data=train_df_step2, x=feature)\n    plt.title(f'Count of Unique Values in {feature}')\n    plt.xlabel('')\n    plt.xticks(rotation=90)\n    plt.ylabel('Counts')\n    \nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 1: Unique values for Employment status\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Unique values for Education level\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Unique values for Marital status\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Unique values for Dwelling\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Unique values for Job title\n\n\n\n\n\n\n Job title has NaN values though, are they all students and pensioners that don’t work anymore? Or are there no job title for some people’s jobs? Let’s combine Employment status and Job title, so we can see what the employment status is of those whose job title is missing.\n\nnan_job_title_df = train_df[train_df['Job title'].isna()]\nemployment_status_counts = nan_job_title_df['Employment status'].value_counts()\n\nplt.figure(figsize=(8, 6))\nplt.pie(employment_status_counts, labels=employment_status_counts.index, autopct='%1.1f%%', startangle=45)\nplt.title('Employment Status for NaN Job Titles')\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\n It’s all around the place. Gathering the information we have so far: - Job title has 9027 NaN values (roughly 30%) - Missing job titles seem to be all over the place compared their employment status, MCAR (Missing Completely at Random) - Imputing would be complicated (but not impossible) - Encoding this feature would increase the dimensionality significantly\nBased on that, I will remove the feature altogether, and apply OHE (One Hot Encoding).\n\ntrain_df_step3 = train_df_step2.drop(['Job title'], axis=1)\n\ntrain_df_step4 = pd.get_dummies(train_df_step3, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)\n\n\n\nCorrelation matrix\nFinally, let’s take a look at the correlation matrix.\n\ncorrelation_matrix = train_df_step4.corr()\n\nplt.figure(figsize=(25,25))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation matrix')\nplt.show()\n\n\n\n\n\n\n\n\n There are two things that might be interesting to explore further: - Correlation between ‘Family member count’ and ‘Children count’. - Correlation between ‘Employment status_Pensioner’ and ‘Employment length’.\n‘Family member count’ and ‘Children count’ are obviously correlated, but it is not a correlation of 1. That means that sometimes, more family members taken into account other than children, or even sometimes children are not taken into account for family, therefore we can’t really remove this feature. It is adding some information after all.\n\nfamily_member_count = train_df_step4['Family member count']\nchildren_count = train_df_step4['Children count']\n\nplt.figure(figsize=(8, 6))\nplt.scatter(family_member_count, children_count, alpha=0.5)\nplt.title('Scatter Plot of Family Member Count vs Children Count')\nplt.xlabel('Family Member Count')\nplt.ylabel('Children Count')\nplt.grid(True)\n\nplt.gca().xaxis.set_major_locator(MultipleLocator(1))\nplt.gca().yaxis.set_major_locator(MultipleLocator(1))\n\nplt.show()\n\n\n\n\n\n\n\n\n We can see that most of the time, children make up for most of the family members, but sometimes (especifically for 1 family member), there are a few cases with more children than family members.\nMoving to the next point to explore, let’s take a look at those with an employment status of pensioner and their respective ‘Employment length’.\n\nemployment_status_pensioner = train_df_step4[(train_df_step4['Employment status_Pensioner'] == 1)]\nemployment_status_pensioner['Employment length'].value_counts()\n\nEmployment length\n 365243    4908\n-678          1\n-443          1\n-586          1\n-672          1\n-673          1\n-3680         1\n-2745         1\n-2208         1\n-620          1\n-1325         1\n-1644         1\n-2269         1\nName: count, dtype: int64\n\n\n Out of 4920 pensioners, only 12 have an ‘Employment length’ different than the value ‘365243’. That is a 0.24% over the pensioners and 0.04% over the dataset. It is most likely safe to remove one of these two features, as they are explaining the same thing. But which one to remove matters: ‘Employment status_Pensioner’ has the same information as ‘Employment length’, but not the other way around. ‘Employment length’ has information about those that are not Pensioners.\nThat means we can only remove the Employment status_Pensioner feature.\n\ntrain_df_step5 = train_df_step4.drop(['Employment status_Pensioner'], axis=1)\n\n\n\nScaling features\nAt last, before training a machine learning model, we should scale features, given that some of them like ‘Income’ and ‘Employment length’ are not proportional to the others.\n\nscaler = MinMaxScaler()\ntrain_df_step6 = scaler.fit_transform(train_df_step5)\ntrain_df_step6 = pd.DataFrame(train_df_step6, columns=train_df_step5.columns)"
  },
  {
    "objectID": "articles/002-Datasets.html#training-a-model",
    "href": "articles/002-Datasets.html#training-a-model",
    "title": "Exploring Datasets",
    "section": "Training a model",
    "text": "Training a model\nTo train a model we first split the data in input (x_data) and output (y_data). Then we apply the following: - Split dataset between train and test sets. - Apply cross validation - Search for best hyperparameters - Make predictions - Calculate accuracy\nTo visually see the results, we can use a confusion matrix.\n\ny_data = train_df_step6['Is high risk']\nx_data = train_df_step6.drop(columns=['Is high risk'])\n\ndef train_model(x, y):\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    rf_classifier = RandomForestClassifier(random_state=42)\n    param_grid = {\n        'n_estimators': [10, 100],\n        'max_depth': [None, 5, 10],\n        'min_samples_split': [2, 4],\n        'min_samples_leaf': [1, 3]\n    }\n\n    grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n    grid_search.fit(X_train, y_train)\n\n    best_params = grid_search.best_params_\n    print(\"Best hyperparameters:\", best_params)\n    best_model = grid_search.best_estimator_\n\n    y_pred = best_model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy on testing set:\", accuracy)\n    \n    y_pred = best_model.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred)\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.show()\n    \n    return best_model\n    \nbest_model = train_model(x_data, y_data)\n\n 98.52% accuracy is great! That meanst almost every observation we made about the dataset were appropiate and helped us build an accurate model… or not.\nSome machine learning models, especially those that are tree based, tend to fall into a trap of unbalanced classes. And in this dataset, there’s a huge unbalance in our output ‘Is high risk’.\n\ntrain_df_step6['Is high risk'].value_counts()\n\n Only 1.74% are high risk. What does that mean? Well… instead of analyzing the dataset, evaluating models, hyperparameters, cross validation and whatnot, it would be more effective to say “No client is high risk”. That would give us an accuracy of 98.26%. Looking at the confusion matrix, that is exactly what the model learned and did.\n\nBalancing classes\nOne way to fix this is to balance the classes, either by undersampling or oversampling. Let’s try and use oversampling, as we don’t really want to loose 28000 entries of valuable information.\n\nsmote = SMOTE()\n\nx_smote, y_smote = smote.fit_resample(x_data, y_data)\n\nprint('Original dataset shape')\nprint(y_data.value_counts())\nprint('\\nResample dataset shape')\nprint(y_smote.value_counts())\n\n Now classes are balanced, let’s train a new model… a balanced one.\n\nbest_model_balanced = train_model(x_smote, y_smote)\n\n\n\nComparing balanced and unbalanced models\nAnd now, after balancing our classes, we get an even better accuracy, 99.03%! But it wouldn’t be fair to compare when we artificially created almost half the data. Let’s compare both models, the balanced and unbalanced models, with the entire dataset as input.\n\ny_pred_balanced = best_model_balanced.predict(x_data)\nbalanced_accuracy = accuracy_score(y_data, y_pred_balanced)\n\ncm = confusion_matrix(y_data, y_pred_balanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\ny_pred_unbalanced = best_model.predict(x_data)\nunbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)\n\ncm = confusion_matrix(y_data, y_pred_unbalanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.show()\n\n Results for the balanced model: - It predicted that 436 cases were high risk, 387 of those were rightfully high risk (88.76% accuracy). - It predicted that 28729 cases were not high risk, 28617 of those were rightfully not high risk (99.61% accuracy). - Overall accuracy of 99.45%.\nResults for the unbalanced mode: - It predicted that 3 cases were high risk, 3 of those were rightfully high risk (100% accuracy). - It predicted that 29162 cases were not high risk, 28666 of those were rightfully not high risk (98.30% accuracy). - Overall accuracy of 98.30%.\nOnly when measured by their overall accuracy does the balanced model truly shine. But only a glimpse into the confusion matrix will clearly shows that the balanced model actually learned from the high risk entries, whereas the unbalanced one barely did.\n\n\nComparing with unseen data\nIn all fairness though, we’re measuring them by almost the same data we trained both models on. Let’s throw at them data it they have never seen.\n\ntest_df = pd.read_csv(\"./test_data.csv\")\ntest_df = pd.get_dummies(test_df, columns=['Employment status', 'Education level', 'Marital status', 'Dwelling'], dtype=int)\ntest_df = test_df.drop(['ID','Has a mobile phone','Job title','Employment status_Pensioner'], axis=1)\nwith pd.option_context(\"future.no_silent_downcasting\", True):\n    test_df['Gender'] = test_df['Gender'].replace({'M': int(0), 'F': int(1)}).astype(int)\n    test_df['Has a property'] = test_df['Has a property'].replace({'N': int(0), 'Y': int(1)}).astype(int)\n    test_df['Has a car'] = test_df['Has a car'].replace({'N': int(0), 'Y': int(1)}).astype(int)\nscaler = MinMaxScaler()\ntest_df_columns = test_df.columns\ntest_df = scaler.fit_transform(test_df)\ntest_df = pd.DataFrame(test_df, columns = test_df_columns)\n\n\n\ny_data = test_df['Is high risk']\nx_data = test_df.drop(columns=['Is high risk'])\n\ny_pred_unbalanced = best_model.predict(x_data)\nunbalanced_accuracy = accuracy_score(y_data, y_pred_unbalanced)\n\ny_pred_balanced = best_model_balanced.predict(x_data)\nbalanced_accuracy = accuracy_score(y_data, y_pred_balanced)\n\nprint(f\"Accuracy on dataset with balanced training: {balanced_accuracy*100:.2f}%\")\nprint(f\"Accuracy on dataset set with unbalanced training: {unbalanced_accuracy*100:.2f}%\")\n\n Overall accuracy of the balanced model dropped to 90.65% and the unbalanced model stayed high at 98.40%, but is it truly accurate?\n\ncm = confusion_matrix(y_data, y_pred_unbalanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Unbalanced Confusion Matrix')\nplt.show()\n\ncm = confusion_matrix(y_data, y_pred_balanced)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Balanced Confusion Matrix')\nplt.show()\n\n It isn’t. The unbalanced model did not predict any high risk, whereas the balanced model predicted 635 cases. It did so correctly at 5.51% accuracy which is not that good, but that is a story for a different day."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nicolas Tertusio",
    "section": "",
    "text": "Electromechanical engineer with experience in construction projects and electrical systems. I possess technical skills for developing blueprints, efficient lighting designs, and programming in various languages for tasks such as web applications and data analysis. Experienced in project management and supervision, including budgeting, quotations, contract payments, and material procurement. I have a keen interest in adhering to standards and utilizing corrective and predictive measurement equipment such as thermography, vibration analysis, and power quality assessments. Currently seeking to bridge my engineering interests with the flourishing field of artificial intelligence.\nTechnical skills: C#, Python, Flutter, Microsoft Office, AutoCAD\nSoft skills: responsible, organized, efficient, proactive\nLanguages: Spanish (native), English (Advanced, C2), Italian (intermedium B1)\n\n\nTechnological University of Panama | Panama, Panama City Bachelor's Degree in Electromechanical Engineering | 2017 - 2023\nUniversity of Buenos Aires | Buenos Aires, Argentina Master's Degree in Artificial Intelligence | Oct 2023 - Present\n\n\n\nEmpresas Melo, S.A. | Project Inspector | Jan 2020 - present"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Nicolas Tertusio",
    "section": "",
    "text": "Technological University of Panama | Panama, Panama City Bachelor's Degree in Electromechanical Engineering | 2017 - 2023\nUniversity of Buenos Aires | Buenos Aires, Argentina Master's Degree in Artificial Intelligence | Oct 2023 - Present"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Nicolas Tertusio",
    "section": "",
    "text": "Empresas Melo, S.A. | Project Inspector | Jan 2020 - present"
  },
  {
    "objectID": "articles/001-Markdown.html",
    "href": "articles/001-Markdown.html",
    "title": "Learning to use Markdown",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\nSee code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\nFigure 1\nMarkdown is a lightweight markup language with plain-text formatting syntax. With it, you can create formatted text using simple symbols and keywords.\nSee code\nimport pandas as pd\nimport seaborn as sns\nimport string\n\n# Seed for reproducibility\nnp.random.seed(42)\n\n# Creating a list of standard deviations, from 10% to 100% of the mean\nstd_devs = np.linspace(0.1 * 50, 1 * 50, 20)  # Varying standard deviations from 10% to 100% of the mean\n\n# Generating the DataFrame with varying standard deviations\nrandom_array = np.array([np.random.normal(50, std_dev, 10000) for std_dev in std_devs]).T  # Transpose to correct shape\n\n# Creating a Pandas DataFrame from the NumPy array\ndf = pd.DataFrame(random_array, columns=list(string.ascii_uppercase[:20]))\ndf\nJupyter Notebooks are interactive computational environments where you can combine code execution, text, mathematics, plots, and rich media. They are widely used in data science, machine learning, and academic research for their flexibility and ease of use.\nIn this article I will be covering the most common formatting techniques used in Markdown."
  },
  {
    "objectID": "articles/001-Markdown.html#text-formatting",
    "href": "articles/001-Markdown.html#text-formatting",
    "title": "Learning to use Markdown",
    "section": "Text formatting",
    "text": "Text formatting\n\nHeaders\nOne can apply header formatting using (#).\n# is a title\n## is a header\n### is a sub header\n#### is a sub sub header\n\n\nEmphasis\nEncapsulating a text in asterisks (*) one can apply italics format:\n*This text will be in italics.*\nThis text is in italics.\nEncapsulating a text in double asterisks (**) one can apply bold format:\n**This text will be bold.**\nThis text is bold.\nEncapsulating a text in double tilde (~) one can apply strikethrough:\n~~This text will be strikethrough.~~\nThis text is strikethrough.\nEncapsulating a text in backticks (`) one can apply coding format:\nThis text is displayed as code.\n`This text will be displayed as code.`\nEncapsulating a text in tripple backticks (```) a line before it begins and a line after it finishes, one can apply multiline coding format:\nThis is the first line of a multiline code.\nThis is the second line of a multiline code.\nThis is the third line of a multiline code.\nUsing a single greater than (&gt;) at the beggining, one can show a quote:\n&gt; This text will be a quote\n\nThis text is a quote"
  },
  {
    "objectID": "articles/001-Markdown.html#lists",
    "href": "articles/001-Markdown.html#lists",
    "title": "Learning to use Markdown",
    "section": "Lists",
    "text": "Lists\n\nUnordered lists\nTo start an unordered list, use a single hyphen and space (-) followed by text. To start an ordered list, use a number, dot and space (1.) followed by text. To add sublists, follow the same syntax but add two spaces before.\nThis input:\n- First bullet point\n    - First sub bullet point\n        - First sub sub bullet point\n        - Second sub sub bullet point\n        - Third sub sub bullet point\n    - Second sub bullet point\n    - Third sub bullet point\n- Second bullet point\n- Third bullet point\nHas this output:\n\nFirst bullet point\n\nFirst sub bullet point\n\nFirst sub sub bullet point\nSecond sub sub bullet point\nThird sub sub bullet point\n\nSecond sub bullet point\nThird sub bullet point\n\nSecond bullet point\nThird bullet point\n\n\n\nOrdered lists\nTo start an ordered list, use a number, dot and space (1.) followed by text. To add sublists, follow the same syntax but add two spaces before.\nThis input:\n1. First points\n    1. First sub point\n        1. First sub sub point\n        2. Second sub sub point\n        3. Third sub sub point\n    2. Second sub point\n    3. Third sub point\n2. Second point\n3. Third point\nHas this output:\n\nFirst points\n\nFirst sub point\n\nFirst sub sub point\nSecond sub sub point\nThird sub sub point\n\nSecond sub point\nThird sub point\n\nSecond point\nThird point"
  },
  {
    "objectID": "articles/001-Markdown.html#hyperlink",
    "href": "articles/001-Markdown.html#hyperlink",
    "title": "Learning to use Markdown",
    "section": "Hyperlink",
    "text": "Hyperlink\n\nExternal links\nLinking to outside websites is done by encapsulating in square brackets ([]) the text that can be clicked to use the link, followed by parenthesis (()) with the website’s URL. One can add a space after the URL and a text encapsulated by quotation marks (\"\") to show a name when the link is hovered over.\n[Text to be clicked](http://google.com \"Hovered information\")\nText to be clicked\n[External Link to Google](http://google.com)\nExternal Link to Google\n\n\nInternal links\nInternal links work the same way as external links, but inside the document. Instead of an URL, a number sign (#) is used followed by the name of the heading in the document to link. In this instance, spaces are replaced by hyphens.\n[This will take you to the text formatting header.](#text-formatting)\nThis will take you to the text formatting header.\n\n\nImages\nImages work the same way as external links, but to display them, an exclamation mark (!) is used before the square brackets ([]).\n![Look at this image!](https://flutter.github.io/assets-for-api-docs/assets/widgets/owl.jpg \"Fancy owl\")"
  },
  {
    "objectID": "articles/001-Markdown.html#mathematical-notation",
    "href": "articles/001-Markdown.html#mathematical-notation",
    "title": "Learning to use Markdown",
    "section": "Mathematical notation",
    "text": "Mathematical notation\nMathematical notation is rendered by using LaTeX.\n\nIt can be used inline by wrapping a mathematical expression with single dollar signs ($), or multiline by wrapping the expressions with double dollar signs ($$).\nSingle line expressions will be left aligned if they are not inside a text block.\nMultiline expressions will automatically be aligned at the center\n\nOne can aligned them at a specific point by wrapping the expression inside the double dollar signs with \\begin{split} and \\end{split}. Place an ampersand (&) where the alignment should be.\n\nAll special mathematical symbols (like roots, powers, underlines, bars, greek letters, summation, integral, fractions, and so on) start by using a backslash and the corresponding name.\n\n\n\n\nInline Expression\nLaTeX\n\n\n\n\n$\\sqrt{x}$\n\\(\\sqrt{x}\\)\n\n\n$x^2$\n\\(x^2\\)\n\n\n$\\underline{x}$\n\\(\\underline{x}\\)\n\n\n$\\bar{x}$\n\\(\\bar{x}\\)\n\n\n$\\sigma$\n\\(\\sigma\\)\n\n\n$\\frac{x+1}{x-1}$\n\\(\\frac{x+1}{x-1}\\)\n\n\n$\\sum_{0}^{\\infty}{\\frac{1}{n}}$\n\\(\\sum_{0}^{\\infty}{\\frac{1}{n}}\\)\n\n\n$\\int_{0}^{\\infty}{\\frac{1}{n}}$\n\\(\\int_{0}^{\\infty}{\\frac{1}{n}}\\)\n\n\n\nMultiline aligned at the equal sign input:\n$$\n\\begin{split}\n\\dot{x} & = \\sigma (3y - 5t)\\\\\n\\dot{x} & = 3 \\sigma y - 5 \\sigma t \\\\\n\\dot{x} & = (1+1+1) \\sigma y - (10-5) \\sigma t \\\\\n\\end{split}\n$$\nOutput: \\[\n\\begin{split}\n\\dot{x} & = \\sigma (3y - 5t)\\\\\n\\dot{x} & = 3 \\sigma y - 5 \\sigma t \\\\\n\\dot{x} & = (1+1+1) \\sigma y - (10-5) \\sigma t \\\\\n\\end{split}\n\\]\nMultiline aligned at the center input:\n$$\n\\begin{gather}\n\\dot{x} = \\sigma (3y - 5t)\\\\\n\\dot{x} = 3 \\sigma y - 5 \\sigma t\\\\\n\\dot{x} = (1+1+1) \\sigma y - (10-5) \\sigma t\\\\\n\\end{gather}\n$$\n\\[\n\\begin{gather}\n\\dot{x} = \\sigma (3y - 5t)\\\\\n\\dot{x} = 3 \\sigma y - 5 \\sigma t\\\\\n\\dot{x} = (1+1+1) \\sigma y - (10-5) \\sigma t\\\\\n\\end{gather}\n\\]"
  },
  {
    "objectID": "articles/001-Markdown.html#colored-boxes",
    "href": "articles/001-Markdown.html#colored-boxes",
    "title": "Learning to use Markdown",
    "section": "Colored boxes",
    "text": "Colored boxes\nColored boxes can be used to highlight important information.\n\nBlue boxes provide useful information (alert-info).\nYellow boxes provide warnings (alert-warning).\nGreen boxes indicate successful outcomes (alert-success).\nRed boxes provide critical information regarding errors or potential mistakes (alert-danger).\n\nThey are a special case and one has to use HTML notation for them. Start by using &lt;div&gt;&lt;/div&gt; with class \"alert alert-info\" (or the corresponding name) and finish by putting the text in between them.\nInput:\n&lt;div class=\"alert alert-info\"&gt;\n&lt;b&gt;Alert info:&lt;/b&gt; Blue boxes (alert-info) provide useful information.\n&lt;/div&gt;\nOutput:\n\nAlert info: Blue boxes (alert-info) provide useful information.\n\nInput:\n&lt;div class=\"alert alert-warning\"&gt;\n&lt;b&gt;Alert warning:&lt;/b&gt; Yellow boxes (alert-warning) provide warnings.\n&lt;/div&gt;\nOutput:\n\nAlert warning: Yellow boxes (alert-warning) provide warnings.\n\nInput:\n&lt;div class=\"alert alert-success\"&gt;\n&lt;b&gt;Alert success:&lt;/b&gt; Green boxes (alert-success) indicate successful outcomes.\n&lt;/div&gt;\nOutput:\n\nAlert success: Green boxes (alert-success) indicate successful outcomes.\n\nInput:\n&lt;div class=\"alert alert-danger\"&gt;\n&lt;b&gt;Alert danger:&lt;/b&gt; Red boxes (alert-danger) provide critical information regarding errors or potential mistakes.\n&lt;/div&gt;\nOutput:\n\nAlert danger: Red boxes (alert-danger) provide critical information regarding errors or potential mistakes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Artificial Intelligence",
    "section": "",
    "text": "This website functions as a portfolio where I document my journey through learning about data science and artificial intelligence, a platform to share my progress, insights, and experiences as I delve into various topics in the field.\nHere, you will find a collection of articles and projects covering a range of subjects, including data visualization, data analysis, computer vision, deep learning, machine learning, and more. Through these posts, I aim to offer valuable information and share the lessons I’ve learned along the way.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nExploring Datasets\n\n\nThe journey from raw data to a predictive model involves several crucial steps.\n\n\n\nMay 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearning to use Markdown\n\n\nThe most common formatting techniques used in Markdown, a lightweight markup language with plain-text formatting syntax.\n\n\n\nMay 7, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]